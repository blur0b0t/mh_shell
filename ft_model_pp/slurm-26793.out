got current job name=fts5
new job name=fts6
new job created with id: 26878
starting fine tuning model
Requirement already satisfied: datasets in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.14.5)
Requirement already satisfied: torch in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.0.1)
Requirement already satisfied: transformers>=4.32.0 in /opt/miniconda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.35.0.dev0)
Requirement already satisfied: sentencepiece in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.99)
Requirement already satisfied: peft in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.5.0)
Requirement already satisfied: evaluate in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.4.0)
Requirement already satisfied: nltk in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.8.1)
Requirement already satisfied: rouge_score in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.1.2)
Requirement already satisfied: einops in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.6.1)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.0)
Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)
Requirement already satisfied: packaging in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.16.4)
Requirement already satisfied: pyarrow>=8.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)
Requirement already satisfied: xxhash in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.3.0)
Requirement already satisfied: aiohttp in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.5)
Requirement already satisfied: requests>=2.19.0 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.28.1)
Requirement already satisfied: multiprocess in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)
Requirement already satisfied: pandas in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.0)
Requirement already satisfied: networkx in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (8.5.0.96)
Requirement already satisfied: jinja2 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.2)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.14.3)
Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.8.0)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.101)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.91)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.12.4)
Requirement already satisfied: sympy in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.4.91)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.9.0.58)
Requirement already satisfied: triton==2.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.0.0)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.10.3.66)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.4.0.1)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.2.10.91)
Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (0.38.4)
Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (65.6.3)
Requirement already satisfied: lit in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (16.0.6)
Requirement already satisfied: cmake in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (3.27.5)
Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (0.14.0)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (2023.8.8)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (0.3.3)
Requirement already satisfied: psutil in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (5.9.5)
Requirement already satisfied: accelerate in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (0.23.0)
Requirement already satisfied: responses<0.19 in /home/u131168/.local/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 6)) (0.18.0)
Requirement already satisfied: click in /home/u131168/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 7)) (8.1.7)
Requirement already satisfied: joblib in /home/u131168/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 7)) (1.3.2)
Requirement already satisfied: absl-py in /home/u131168/.local/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (2.0.0)
Requirement already satisfied: six>=1.14.0 in /opt/miniconda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (1.16.0)
Requirement already satisfied: attrs>=17.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)
Requirement already satisfied: frozenlist>=1.1.1 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.2)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/miniconda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2023.5.7)
Requirement already satisfied: MarkupSafe>=2.0 in /home/u131168/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)
Requirement already satisfied: pytz>=2020.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)
Requirement already satisfied: mpmath>=0.19 in /home/u131168/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-d4daae8a
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-d4daae8a
  Resolved https://github.com/huggingface/transformers to commit 19f0b7dd02c7ea7cbf86cc87fe00667470266722
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.14.0)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (6.0.1)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (1.26.0)
Requirement already satisfied: tqdm>=4.27 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (4.65.0)
Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (2.28.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.16.4)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (2023.8.8)
Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (23.0)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (3.12.4)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.35.0.dev0) (0.3.3)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (4.8.0)
Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0.dev0) (2023.6.0)
Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (2.0.4)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (2023.5.7)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.35.0.dev0) (1.26.15)
Requirement already satisfied: tokenizers in /home/u131168/.local/lib/python3.10/site-packages (0.14.0)
Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /home/u131168/.local/lib/python3.10/site-packages (from tokenizers) (0.16.4)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (6.0.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.8.0)
Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (4.65.0)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (3.12.4)
Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2.28.1)
Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.6.0)
Requirement already satisfied: packaging>=20.9 in /opt/miniconda/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers) (23.0)
Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2.0.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (1.26.15)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (2023.5.7)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers) (3.4)
/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9200
/opt/miniconda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
  warn("The installed version of bitsandbytes was compiled without GPU support. "
/opt/miniconda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
-------------------starting-script
-------------------setting up-logging
10/05/2023 03:54:33 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
10/05/2023 03:54:33 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/runs/Oct05_03-54-33_idc-beta-batch-pvc-node-10,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=1.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9200,
run_name=/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/,
save_on_each_node=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
/home/u131168/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
Using custom data configuration default-0b98ef580560d6a3
----------------------detecting checkpoint
10/05/2023 03:54:33 - INFO - datasets.builder - Using custom data configuration default-0b98ef580560d6a3
Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
10/05/2023 03:54:33 - INFO - datasets.info - Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
Overwrite dataset info from restored data version if exists.
10/05/2023 03:54:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
10/05/2023 03:54:33 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
10/05/2023 03:54:33 - INFO - datasets.builder - Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
10/05/2023 03:54:33 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:54:33,967 >> loading file spiece.model from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/spiece.model
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:54:33,967 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:54:33,967 >> loading file special_tokens_map.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/special_tokens_map.json
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:54:33,967 >> loading file tokenizer_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2043] 2023-10-05 03:54:33,967 >> loading file tokenizer.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer.json
[WARNING|logging.py:305] 2023-10-05 03:54:34,053 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,235 >> Adding </s> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,235 >> Adding <unk> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,235 >> Adding <pad> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,235 >> Adding <extra_id_0> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,236 >> Adding <extra_id_1> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,236 >> Adding <extra_id_2> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,236 >> Adding <extra_id_3> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,236 >> Adding <extra_id_4> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,236 >> Adding <extra_id_5> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_6> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_7> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_8> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_9> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_10> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,237 >> Adding <extra_id_11> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_12> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_13> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_14> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_15> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_16> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,238 >> Adding <extra_id_17> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_18> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_19> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_20> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_21> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_22> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,239 >> Adding <extra_id_23> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_24> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_25> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_26> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_27> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_28> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,240 >> Adding <extra_id_29> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_30> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_31> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_32> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_33> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_34> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,241 >> Adding <extra_id_35> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_36> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_37> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_38> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_39> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_40> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,242 >> Adding <extra_id_41> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_42> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_43> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_44> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_45> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_46> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,243 >> Adding <extra_id_47> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_48> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_49> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_50> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_51> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_52> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,244 >> Adding <extra_id_53> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,245 >> Adding <extra_id_54> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,245 >> Adding <extra_id_55> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,245 >> Adding <extra_id_56> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,245 >> Adding <extra_id_57> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,245 >> Adding <extra_id_58> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_59> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_60> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_61> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_62> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_63> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,246 >> Adding <extra_id_64> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_65> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_66> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_67> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_68> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_69> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,247 >> Adding <extra_id_70> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_71> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_72> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_73> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_74> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_75> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,248 >> Adding <extra_id_76> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_77> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_78> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_79> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_80> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_81> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,249 >> Adding <extra_id_82> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_83> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_84> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_85> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_86> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_87> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,250 >> Adding <extra_id_88> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_89> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_90> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_91> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_92> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_93> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,251 >> Adding <extra_id_94> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,252 >> Adding <extra_id_95> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,252 >> Adding <extra_id_96> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,252 >> Adding <extra_id_97> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,252 >> Adding <extra_id_98> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,252 >> Adding <extra_id_99> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <pad> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding </s> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <unk> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <extra_id_99> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <extra_id_98> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <extra_id_97> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <extra_id_96> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,377 >> Adding <extra_id_95> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_94> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_93> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_92> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_91> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_90> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_89> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_88> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_87> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_86> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_85> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_84> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_83> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_82> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_81> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_80> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_79> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_78> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_77> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_76> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_75> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_74> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_73> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_72> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_71> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_70> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_69> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_68> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_67> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_66> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_65> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_64> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_63> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_62> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_61> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_60> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_59> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,378 >> Adding <extra_id_58> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_57> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_56> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_55> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_54> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_53> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_52> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_51> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_50> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_49> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_48> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_47> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_46> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_45> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_44> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_43> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_42> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_41> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_40> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_39> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_38> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_37> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_36> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_35> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_34> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_33> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_32> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_31> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_30> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_29> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_28> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_27> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_26> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_25> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_24> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_23> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_22> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,379 >> Adding <extra_id_21> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_20> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_19> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_18> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_17> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_16> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_15> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_14> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_13> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_12> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_11> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_10> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_9> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_8> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_7> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_6> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_5> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_4> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_3> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_2> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_1> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,380 >> Adding <extra_id_0> to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,460 >> Adding ` to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,538 >> Adding ~ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,614 >> Adding ! to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,692 >> Adding @ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,770 >> Adding # to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,848 >> Adding $ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:34,926 >> Adding % to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,003 >> Adding ^ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,082 >> Adding & to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,160 >> Adding * to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,237 >> Adding ( to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,316 >> Adding ) to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,393 >> Adding - to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,470 >> Adding _ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,547 >> Adding + to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,625 >> Adding = to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,701 >> Adding { to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,778 >> Adding } to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,855 >> Adding [ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:35,932 >> Adding ] to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,010 >> Adding | to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,092 >> Adding \ to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,171 >> Adding : to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,248 >> Adding ; to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,326 >> Adding " to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,404 >> Adding ' to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,482 >> Adding < to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,560 >> Adding > to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,638 >> Adding ? to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,715 >> Adding / to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,793 >> Adding 
 to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,870 >> Adding 	 to the vocabulary
[INFO|tokenization_utils.py:493] 2023-10-05 03:54:36,949 >> Adding   to the vocabulary
------------------modifying t5 tokenizer-----------------
Running tokenizer on train dataset:   0%|          | 0/139077 [00:00<?, ? examples/s]Caching processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-ac6cff7f6a1d6659.arrow
-------------------preparing features
10/05/2023 03:54:39 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-0b98ef580560d6a3/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-ac6cff7f6a1d6659.arrow
Running tokenizer on train dataset:   1%|          | 1000/139077 [00:02<04:54, 468.64 examples/s]Running tokenizer on train dataset:   1%|▏         | 2000/139077 [00:04<04:50, 471.25 examples/s]Running tokenizer on train dataset:   2%|▏         | 3000/139077 [00:05<04:20, 521.73 examples/s]Running tokenizer on train dataset:   3%|▎         | 4000/139077 [00:07<04:25, 507.91 examples/s]Running tokenizer on train dataset:   4%|▎         | 5000/139077 [00:10<04:31, 494.68 examples/s]Running tokenizer on train dataset:   4%|▍         | 6000/139077 [00:12<04:31, 489.77 examples/s]Running tokenizer on train dataset:   5%|▌         | 7000/139077 [00:14<04:30, 488.51 examples/s]Running tokenizer on train dataset:   6%|▌         | 8000/139077 [00:15<04:05, 534.12 examples/s]Running tokenizer on train dataset:   6%|▋         | 9000/139077 [00:16<03:31, 613.89 examples/s]Running tokenizer on train dataset:   7%|▋         | 10000/139077 [00:17<03:07, 687.46 examples/s]Running tokenizer on train dataset:   8%|▊         | 11000/139077 [00:18<02:51, 748.48 examples/s]Running tokenizer on train dataset:   9%|▊         | 12000/139077 [00:20<02:39, 794.43 examples/s]Running tokenizer on train dataset:   9%|▉         | 13000/139077 [00:21<02:32, 828.58 examples/s]Running tokenizer on train dataset:  10%|█         | 14000/139077 [00:22<02:25, 857.01 examples/s]Running tokenizer on train dataset:  11%|█         | 15000/139077 [00:23<02:21, 878.93 examples/s]Running tokenizer on train dataset:  12%|█▏        | 16000/139077 [00:24<02:18, 891.29 examples/s]Running tokenizer on train dataset:  12%|█▏        | 17000/139077 [00:26<02:49, 719.87 examples/s]Running tokenizer on train dataset:  13%|█▎        | 18000/139077 [00:28<03:01, 667.94 examples/s]Running tokenizer on train dataset:  14%|█▎        | 19000/139077 [00:29<02:54, 686.54 examples/s]Running tokenizer on train dataset:  14%|█▍        | 20000/139077 [00:31<03:14, 612.97 examples/s]Running tokenizer on train dataset:  15%|█▌        | 21000/139077 [00:33<03:27, 568.07 examples/s]Running tokenizer on train dataset:  16%|█▌        | 22000/139077 [00:35<03:27, 565.10 examples/s]Running tokenizer on train dataset:  17%|█▋        | 23000/139077 [00:36<03:02, 636.09 examples/s]Running tokenizer on train dataset:  17%|█▋        | 24000/139077 [00:37<02:44, 701.41 examples/s]Running tokenizer on train dataset:  18%|█▊        | 25000/139077 [00:38<02:30, 758.60 examples/s]Running tokenizer on train dataset:  19%|█▊        | 26000/139077 [00:39<02:20, 803.49 examples/s]Running tokenizer on train dataset:  19%|█▉        | 27000/139077 [00:41<02:38, 708.08 examples/s]Running tokenizer on train dataset:  20%|██        | 28000/139077 [00:43<03:00, 614.80 examples/s]Running tokenizer on train dataset:  21%|██        | 29000/139077 [00:45<03:15, 563.06 examples/s]Running tokenizer on train dataset:  22%|██▏       | 30000/139077 [00:47<03:05, 587.87 examples/s]Running tokenizer on train dataset:  22%|██▏       | 31000/139077 [00:48<02:44, 656.32 examples/s]Running tokenizer on train dataset:  23%|██▎       | 32000/139077 [00:49<02:28, 720.65 examples/s]Running tokenizer on train dataset:  24%|██▎       | 33000/139077 [00:50<02:16, 775.86 examples/s]Running tokenizer on train dataset:  24%|██▍       | 34000/139077 [00:51<02:09, 812.28 examples/s]Running tokenizer on train dataset:  25%|██▌       | 35000/139077 [00:52<02:03, 843.75 examples/s]Running tokenizer on train dataset:  26%|██▌       | 36000/139077 [00:53<01:59, 863.88 examples/s]Running tokenizer on train dataset:  27%|██▋       | 37000/139077 [00:54<01:56, 877.90 examples/s]Running tokenizer on train dataset:  27%|██▋       | 38000/139077 [00:56<02:20, 718.49 examples/s]Running tokenizer on train dataset:  28%|██▊       | 39000/139077 [00:58<02:40, 623.55 examples/s]Running tokenizer on train dataset:  29%|██▉       | 40000/139077 [01:01<02:53, 572.19 examples/s]Running tokenizer on train dataset:  29%|██▉       | 41000/139077 [01:03<03:02, 537.59 examples/s]Running tokenizer on train dataset:  30%|███       | 42000/139077 [01:05<03:05, 522.64 examples/s]Running tokenizer on train dataset:  31%|███       | 43000/139077 [01:06<02:39, 601.60 examples/s]Running tokenizer on train dataset:  32%|███▏      | 44000/139077 [01:07<02:32, 623.46 examples/s]Running tokenizer on train dataset:  32%|███▏      | 45000/139077 [01:09<02:46, 565.62 examples/s]Running tokenizer on train dataset:  33%|███▎      | 46000/139077 [01:12<02:54, 534.33 examples/s]Running tokenizer on train dataset:  34%|███▍      | 47000/139077 [01:14<02:58, 515.73 examples/s]Running tokenizer on train dataset:  35%|███▍      | 48000/139077 [01:16<03:02, 499.51 examples/s]Running tokenizer on train dataset:  35%|███▌      | 49000/139077 [01:18<03:00, 499.75 examples/s]Running tokenizer on train dataset:  36%|███▌      | 50000/139077 [01:20<03:00, 492.61 examples/s]Running tokenizer on train dataset:  37%|███▋      | 51000/139077 [01:22<03:00, 487.36 examples/s]Running tokenizer on train dataset:  37%|███▋      | 52000/139077 [01:24<03:00, 482.92 examples/s]Running tokenizer on train dataset:  38%|███▊      | 53000/139077 [01:26<02:51, 500.84 examples/s]Running tokenizer on train dataset:  39%|███▉      | 54000/139077 [01:28<02:42, 522.85 examples/s]Running tokenizer on train dataset:  40%|███▉      | 55000/139077 [01:30<02:45, 507.52 examples/s]Running tokenizer on train dataset:  40%|████      | 56000/139077 [01:32<02:49, 489.84 examples/s]Running tokenizer on train dataset:  41%|████      | 57000/139077 [01:33<02:35, 529.38 examples/s]Running tokenizer on train dataset:  42%|████▏     | 58000/139077 [01:35<02:18, 585.44 examples/s]Running tokenizer on train dataset:  42%|████▏     | 59000/139077 [01:36<02:02, 653.26 examples/s]Running tokenizer on train dataset:  43%|████▎     | 60000/139077 [01:37<01:51, 708.37 examples/s]Running tokenizer on train dataset:  44%|████▍     | 61000/139077 [01:38<01:43, 756.12 examples/s]Running tokenizer on train dataset:  45%|████▍     | 62000/139077 [01:39<01:39, 774.53 examples/s]Running tokenizer on train dataset:  45%|████▌     | 63000/139077 [01:41<01:48, 700.41 examples/s]Running tokenizer on train dataset:  46%|████▌     | 64000/139077 [01:43<02:03, 605.60 examples/s]Running tokenizer on train dataset:  47%|████▋     | 65000/139077 [01:45<02:13, 556.35 examples/s]Running tokenizer on train dataset:  47%|████▋     | 66000/139077 [01:47<02:18, 528.45 examples/s]Running tokenizer on train dataset:  48%|████▊     | 67000/139077 [01:50<02:22, 505.34 examples/s]Running tokenizer on train dataset:  49%|████▉     | 68000/139077 [01:52<02:24, 492.38 examples/s]Running tokenizer on train dataset:  50%|████▉     | 69000/139077 [01:54<02:24, 484.78 examples/s]Running tokenizer on train dataset:  50%|█████     | 70000/139077 [01:56<02:23, 480.09 examples/s]Running tokenizer on train dataset:  51%|█████     | 71000/139077 [01:58<02:23, 474.08 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 72000/139077 [02:00<02:21, 475.64 examples/s]Running tokenizer on train dataset:  52%|█████▏    | 73000/139077 [02:02<02:18, 476.34 examples/s]Running tokenizer on train dataset:  53%|█████▎    | 74000/139077 [02:05<02:17, 473.33 examples/s]Running tokenizer on train dataset:  54%|█████▍    | 75000/139077 [02:07<02:17, 465.23 examples/s]Running tokenizer on train dataset:  55%|█████▍    | 76000/139077 [02:09<02:16, 463.73 examples/s]Running tokenizer on train dataset:  55%|█████▌    | 77000/139077 [02:11<02:13, 465.83 examples/s]Running tokenizer on train dataset:  56%|█████▌    | 78000/139077 [02:13<02:11, 462.97 examples/s]Running tokenizer on train dataset:  57%|█████▋    | 79000/139077 [02:15<02:09, 463.53 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 80000/139077 [02:18<02:07, 464.83 examples/s]Running tokenizer on train dataset:  58%|█████▊    | 81000/139077 [02:20<02:04, 464.98 examples/s]Running tokenizer on train dataset:  59%|█████▉    | 82000/139077 [02:22<02:03, 462.85 examples/s]Running tokenizer on train dataset:  60%|█████▉    | 83000/139077 [02:24<02:00, 464.96 examples/s]Running tokenizer on train dataset:  60%|██████    | 84000/139077 [02:26<01:57, 466.77 examples/s]Running tokenizer on train dataset:  61%|██████    | 85000/139077 [02:28<01:56, 465.45 examples/s]Running tokenizer on train dataset:  62%|██████▏   | 86000/139077 [02:31<01:54, 463.18 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 87000/139077 [02:33<01:52, 464.22 examples/s]Running tokenizer on train dataset:  63%|██████▎   | 88000/139077 [02:35<01:49, 465.88 examples/s]Running tokenizer on train dataset:  64%|██████▍   | 89000/139077 [02:37<01:47, 467.26 examples/s]Running tokenizer on train dataset:  65%|██████▍   | 90000/139077 [02:39<01:44, 469.59 examples/s]Running tokenizer on train dataset:  65%|██████▌   | 91000/139077 [02:41<01:42, 468.76 examples/s]Running tokenizer on train dataset:  66%|██████▌   | 92000/139077 [02:43<01:40, 467.71 examples/s]Running tokenizer on train dataset:  67%|██████▋   | 93000/139077 [02:46<01:39, 464.48 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 94000/139077 [02:48<01:36, 466.35 examples/s]Running tokenizer on train dataset:  68%|██████▊   | 95000/139077 [02:50<01:34, 467.85 examples/s]Running tokenizer on train dataset:  69%|██████▉   | 96000/139077 [02:52<01:32, 466.02 examples/s]Running tokenizer on train dataset:  70%|██████▉   | 97000/139077 [02:54<01:30, 463.63 examples/s]Running tokenizer on train dataset:  70%|███████   | 98000/139077 [02:56<01:28, 464.17 examples/s]Running tokenizer on train dataset:  71%|███████   | 99000/139077 [02:58<01:26, 465.45 examples/s]Running tokenizer on train dataset:  72%|███████▏  | 100000/139077 [03:01<01:24, 463.87 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 101000/139077 [03:03<01:22, 463.72 examples/s]Running tokenizer on train dataset:  73%|███████▎  | 102000/139077 [03:05<01:19, 465.19 examples/s]Running tokenizer on train dataset:  74%|███████▍  | 103000/139077 [03:07<01:17, 465.05 examples/s]Running tokenizer on train dataset:  75%|███████▍  | 104000/139077 [03:09<01:15, 461.96 examples/s]Running tokenizer on train dataset:  75%|███████▌  | 105000/139077 [03:11<01:13, 463.49 examples/s]Running tokenizer on train dataset:  76%|███████▌  | 106000/139077 [03:13<01:11, 465.37 examples/s]Running tokenizer on train dataset:  77%|███████▋  | 107000/139077 [03:16<01:09, 463.03 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 108000/139077 [03:18<01:07, 461.36 examples/s]Running tokenizer on train dataset:  78%|███████▊  | 109000/139077 [03:20<01:05, 461.44 examples/s]Running tokenizer on train dataset:  79%|███████▉  | 110000/139077 [03:22<01:02, 462.10 examples/s]Running tokenizer on train dataset:  80%|███████▉  | 111000/139077 [03:24<01:00, 460.40 examples/s]Running tokenizer on train dataset:  81%|████████  | 112000/139077 [03:27<00:58, 460.63 examples/s]Running tokenizer on train dataset:  81%|████████  | 113000/139077 [03:29<00:56, 461.84 examples/s]Running tokenizer on train dataset:  82%|████████▏ | 114000/139077 [03:31<00:54, 461.46 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 115000/139077 [03:33<00:52, 459.29 examples/s]Running tokenizer on train dataset:  83%|████████▎ | 116000/139077 [03:35<00:50, 460.87 examples/s]Running tokenizer on train dataset:  84%|████████▍ | 117000/139077 [03:37<00:47, 463.49 examples/s]Running tokenizer on train dataset:  85%|████████▍ | 118000/139077 [03:39<00:43, 483.54 examples/s]Running tokenizer on train dataset:  86%|████████▌ | 119000/139077 [03:41<00:40, 495.03 examples/s]Running tokenizer on train dataset:  86%|████████▋ | 120000/139077 [03:43<00:39, 481.81 examples/s]Running tokenizer on train dataset:  87%|████████▋ | 121000/139077 [03:46<00:38, 471.89 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 122000/139077 [03:48<00:36, 464.06 examples/s]Running tokenizer on train dataset:  88%|████████▊ | 123000/139077 [03:50<00:34, 461.35 examples/s]Running tokenizer on train dataset:  89%|████████▉ | 124000/139077 [03:52<00:32, 458.69 examples/s]Running tokenizer on train dataset:  90%|████████▉ | 125000/139077 [03:54<00:30, 456.52 examples/s]Running tokenizer on train dataset:  91%|█████████ | 126000/139077 [03:56<00:26, 493.54 examples/s]Running tokenizer on train dataset:  91%|█████████▏| 127000/139077 [03:58<00:25, 483.03 examples/s]Running tokenizer on train dataset:  92%|█████████▏| 128000/139077 [04:00<00:23, 475.44 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 129000/139077 [04:03<00:21, 465.92 examples/s]Running tokenizer on train dataset:  93%|█████████▎| 130000/139077 [04:05<00:19, 469.21 examples/s]Running tokenizer on train dataset:  94%|█████████▍| 131000/139077 [04:06<00:14, 547.38 examples/s]Running tokenizer on train dataset:  95%|█████████▍| 132000/139077 [04:07<00:11, 620.88 examples/s]Running tokenizer on train dataset:  96%|█████████▌| 133000/139077 [04:08<00:08, 681.06 examples/s]Running tokenizer on train dataset:  96%|█████████▋| 134000/139077 [04:09<00:06, 734.53 examples/s]Running tokenizer on train dataset:  97%|█████████▋| 135000/139077 [04:11<00:06, 667.51 examples/s]Running tokenizer on train dataset:  98%|█████████▊| 136000/139077 [04:13<00:05, 585.56 examples/s]Running tokenizer on train dataset:  99%|█████████▊| 137000/139077 [04:15<00:03, 536.00 examples/s]Running tokenizer on train dataset:  99%|█████████▉| 138000/139077 [04:18<00:02, 510.56 examples/s]Running tokenizer on train dataset: 100%|█████████▉| 139000/139077 [04:20<00:00, 494.74 examples/s]Running tokenizer on train dataset: 100%|██████████| 139077/139077 [04:20<00:00, 492.46 examples/s]Running tokenizer on train dataset: 100%|██████████| 139077/139077 [04:21<00:00, 532.68 examples/s]
[INFO|configuration_utils.py:715] 2023-10-05 03:58:58,917 >> loading configuration file config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/config.json
[INFO|configuration_utils.py:775] 2023-10-05 03:58:58,924 >> Model config T5Config {
  "_name_or_path": "google/flan-t5-xl",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.35.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:2993] 2023-10-05 03:58:58,952 >> loading weights file pytorch_model.bin from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/pytorch_model.bin.index.json
[INFO|configuration_utils.py:770] 2023-10-05 03:58:58,964 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
-------------------preparing features
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:26<00:26, 26.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 14.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:31<00:00, 15.92s/it]
[INFO|modeling_utils.py:3775] 2023-10-05 03:59:56,759 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.

[INFO|modeling_utils.py:3783] 2023-10-05 03:59:56,759 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-xl.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:730] 2023-10-05 03:59:56,867 >> loading configuration file generation_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/generation_config.json
[INFO|configuration_utils.py:770] 2023-10-05 03:59:56,868 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

[INFO|modeling_utils.py:1617] 2023-10-05 03:59:56,955 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32110. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[INFO|modeling_utils.py:1617] 2023-10-05 04:00:13,616 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32110. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[INFO|trainer.py:2123] 2023-10-05 04:00:13,684 >> Loading model from /home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9200.
[INFO|trainer.py:1760] 2023-10-05 04:00:37,771 >> ***** Running training *****
[INFO|trainer.py:1761] 2023-10-05 04:00:37,772 >>   Num examples = 139,077
[INFO|trainer.py:1762] 2023-10-05 04:00:37,772 >>   Num Epochs = 1
[INFO|trainer.py:1763] 2023-10-05 04:00:37,772 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1766] 2023-10-05 04:00:37,772 >>   Total train batch size (w. parallel, distributed & accumulation) = 2
[INFO|trainer.py:1767] 2023-10-05 04:00:37,772 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1768] 2023-10-05 04:00:37,772 >>   Total optimization steps = 69,539
[INFO|trainer.py:1769] 2023-10-05 04:00:37,783 >>   Number of trainable parameters = 4,718,592
[INFO|trainer.py:1789] 2023-10-05 04:00:37,789 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1790] 2023-10-05 04:00:37,790 >>   Continuing training from epoch 0
[INFO|trainer.py:1791] 2023-10-05 04:00:37,790 >>   Continuing training from global step 9200
[INFO|trainer.py:1793] 2023-10-05 04:00:37,790 >>   Will skip the first 0 epochs then the first 9200 batches in the first epoch.
trainable params: 4,718,592 || all params: 2,854,402,048 || trainable%: 0.16530929843279035
-------------------traing-model
  0%|          | 0/69539 [00:00<?, ?it/s] 13%|█▎        | 9201/69539 [09:19<1:01:07, 16.45it/s] 13%|█▎        | 9201/69539 [09:29<1:01:07, 16.45it/s] 13%|█▎        | 9202/69539 [18:09<2:23:57,  6.99it/s] 13%|█▎        | 9203/69539 [24:40<3:51:06,  4.35it/s] 13%|█▎        | 9204/69539 [30:46<5:47:27,  2.89it/s] 13%|█▎        | 9205/69539 [36:55<8:35:10,  1.95it/s] 13%|█▎        | 9206/69539 [43:05<12:34:47,  1.33it/s] 13%|█▎        | 9207/69539 [49:23<18:24:14,  1.10s/it] 13%|█▎        | 9208/69539 [58:00<29:46:08,  1.78s/it] 13%|█▎        | 9209/69539 [1:06:35<45:53:56,  2.74s/it] 13%|█▎        | 9210/69539 [1:07:16<47:34:19,  2.84s/it]                                                          13%|█▎        | 9210/69539 [1:07:16<47:34:19,  2.84s/it] 13%|█▎        | 9211/69539 [1:07:21<47:45:34,  2.85s/it] 13%|█▎        | 9212/69539 [1:07:27<47:58:52,  2.86s/it] 13%|█▎        | 9213/69539 [1:07:31<48:11:27,  2.88s/it] 13%|█▎        | 9214/69539 [1:07:33<48:00:55,  2.87s/it] 13%|█▎        | 9215/69539 [1:07:36<47:56:13,  2.86s/it] 13%|█▎        | 9216/69539 [1:07:38<47:49:46,  2.85s/it] 13%|█▎        | 9217/69539 [1:07:40<47:18:24,  2.82s/it] 13%|█▎        | 9218/69539 [1:07:42<46:31:13,  2.78s/it] 13%|█▎        | 9219/69539 [1:07:43<45:33:34,  2.72s/it] 13%|█▎        | 9220/69539 [1:07:47<46:06:11,  2.75s/it]                                                          13%|█▎        | 9220/69539 [1:07:47<46:06:11,  2.75s/it] 13%|█▎        | 9221/69539 [1:07:49<45:14:47,  2.70s/it] 13%|█▎        | 9222/69539 [1:07:57<55:53:40,  3.34s/it] 13%|█▎        | 9223/69539 [1:08:01<57:26:15,  3.43s/it] 13%|█▎        | 9224/69539 [1:08:03<55:08:28,  3.29s/it] 13%|█▎        | 9225/69539 [1:08:05<49:49:28,  2.97s/it] 13%|█▎        | 9226/69539 [1:08:08<48:07:15,  2.87s/it] 13%|█▎        | 9227/69539 [1:08:11<49:43:20,  2.97s/it] 13%|█▎        | 9228/69539 [1:08:13<44:41:31,  2.67s/it] 13%|█▎        | 9229/69539 [1:08:15<40:46:41,  2.43s/it] 13%|█▎        | 9230/69539 [1:08:16<38:43:46,  2.31s/it]                                                          13%|█▎        | 9230/69539 [1:08:16<38:43:46,  2.31s/it] 13%|█▎        | 9231/69539 [1:08:19<39:39:26,  2.37s/it] 13%|█▎        | 9232/69539 [1:08:21<35:30:27,  2.12s/it] 13%|█▎        | 9233/69539 [1:08:23<35:14:31,  2.10s/it] 13%|█▎        | 9234/69539 [1:08:25<35:15:34,  2.10s/it] 13%|█▎        | 9235/69539 [1:08:27<35:10:35,  2.10s/it] 13%|█▎        | 9236/69539 [1:08:29<34:20:04,  2.05s/it] 13%|█▎        | 9237/69539 [1:08:30<32:55:18,  1.97s/it] 13%|█▎        | 9238/69539 [1:08:33<34:26:09,  2.06s/it] 13%|█▎        | 9239/69539 [1:08:36<38:15:31,  2.28s/it] 13%|█▎        | 9240/69539 [1:08:38<40:31:57,  2.42s/it]                                                          13%|█▎        | 9240/69539 [1:08:38<40:31:57,  2.42s/it] 13%|█▎        | 9241/69539 [1:08:41<40:44:08,  2.43s/it] 13%|█▎        | 9242/69539 [1:08:42<36:37:35,  2.19s/it] 13%|█▎        | 9243/69539 [1:08:44<33:51:06,  2.02s/it] 13%|█▎        | 9244/69539 [1:08:46<34:12:19,  2.04s/it] 13%|█▎        | 9245/69539 [1:08:53<59:00:45,  3.52s/it] 13%|█▎        | 9246/69539 [1:08:57<63:21:47,  3.78s/it] 13%|█▎        | 9247/69539 [1:09:02<65:40:31,  3.92s/it] 13%|█▎        | 9248/69539 [1:09:04<58:26:13,  3.49s/it] 13%|█▎        | 9249/69539 [1:09:05<47:28:36,  2.83s/it] 13%|█▎        | 9250/69539 [1:09:07<41:22:50,  2.47s/it]                                                          13%|█▎        | 9250/69539 [1:09:07<41:22:50,  2.47s/it] 13%|█▎        | 9251/69539 [1:09:12<54:05:00,  3.23s/it] 13%|█▎        | 9252/69539 [1:09:14<49:30:31,  2.96s/it] 13%|█▎        | 9253/69539 [1:09:16<44:07:56,  2.64s/it] 13%|█▎        | 9254/69539 [1:09:18<39:12:30,  2.34s/it] 13%|█▎        | 9255/69539 [1:09:19<34:56:58,  2.09s/it] 13%|█▎        | 9256/69539 [1:09:21<33:34:42,  2.01s/it] 13%|█▎        | 9257/69539 [1:09:22<29:22:31,  1.75s/it] 13%|█▎        | 9258/69539 [1:09:24<29:05:59,  1.74s/it] 13%|█▎        | 9259/69539 [1:09:26<30:25:02,  1.82s/it] 13%|█▎        | 9260/69539 [1:09:28<29:03:06,  1.74s/it]                                                          13%|█▎        | 9260/69539 [1:09:28<29:03:06,  1.74s/it] 13%|█▎        | 9261/69539 [1:09:29<27:28:05,  1.64s/it] 13%|█▎        | 9262/69539 [1:09:31<27:15:09,  1.63s/it] 13%|█▎        | 9263/69539 [1:09:32<26:58:45,  1.61s/it] 13%|█▎        | 9264/69539 [1:09:34<26:22:49,  1.58s/it] 13%|█▎        | 9265/69539 [1:09:36<28:31:46,  1.70s/it] 13%|█▎        | 9266/69539 [1:09:37<27:32:51,  1.65s/it] 13%|█▎        | 9267/69539 [1:09:39<29:39:54,  1.77s/it] 13%|█▎        | 9268/69539 [1:09:42<34:39:20,  2.07s/it] 13%|█▎        | 9269/69539 [1:09:44<33:57:21,  2.03s/it] 13%|█▎        | 9270/69539 [1:09:46<35:01:14,  2.09s/it]                                                          13%|█▎        | 9270/69539 [1:09:46<35:01:14,  2.09s/it] 13%|█▎        | 9271/69539 [1:09:48<32:49:19,  1.96s/it] 13%|█▎        | 9272/69539 [1:09:50<32:09:37,  1.92s/it] 13%|█▎        | 9273/69539 [1:09:52<34:21:23,  2.05s/it] 13%|█▎        | 9274/69539 [1:09:54<32:54:52,  1.97s/it] 13%|█▎        | 9275/69539 [1:09:55<30:46:53,  1.84s/it] 13%|█▎        | 9276/69539 [1:09:57<30:40:07,  1.83s/it] 13%|█▎        | 9277/69539 [1:09:59<30:47:31,  1.84s/it] 13%|█▎        | 9278/69539 [1:10:02<35:28:08,  2.12s/it] 13%|█▎        | 9279/69539 [1:10:04<35:29:50,  2.12s/it] 13%|█▎        | 9280/69539 [1:10:05<31:30:58,  1.88s/it]                                                          13%|█▎        | 9280/69539 [1:10:05<31:30:58,  1.88s/it] 13%|█▎        | 9281/69539 [1:10:07<31:16:29,  1.87s/it] 13%|█▎        | 9282/69539 [1:10:10<33:45:45,  2.02s/it] 13%|█▎        | 9283/69539 [1:10:12<37:03:05,  2.21s/it] 13%|█▎        | 9284/69539 [1:10:14<35:22:50,  2.11s/it] 13%|█▎        | 9285/69539 [1:10:16<31:59:40,  1.91s/it] 13%|█▎        | 9286/69539 [1:10:17<29:15:10,  1.75s/it] 13%|█▎        | 9287/69539 [1:10:20<35:02:08,  2.09s/it] 13%|█▎        | 9288/69539 [1:10:21<31:34:58,  1.89s/it] 13%|█▎        | 9289/69539 [1:10:24<36:04:44,  2.16s/it] 13%|█▎        | 9290/69539 [1:10:26<34:35:02,  2.07s/it]                                                          13%|█▎        | 9290/69539 [1:10:26<34:35:02,  2.07s/it] 13%|█▎        | 9291/69539 [1:10:28<34:24:24,  2.06s/it] 13%|█▎        | 9292/69539 [1:10:30<34:19:25,  2.05s/it] 13%|█▎        | 9293/69539 [1:10:32<32:19:08,  1.93s/it] 13%|█▎        | 9294/69539 [1:10:33<31:39:11,  1.89s/it] 13%|█▎        | 9295/69539 [1:10:35<29:06:25,  1.74s/it] 13%|█▎        | 9296/69539 [1:10:38<35:51:18,  2.14s/it] 13%|█▎        | 9297/69539 [1:10:39<31:22:25,  1.87s/it] 13%|█▎        | 9298/69539 [1:10:41<29:35:15,  1.77s/it] 13%|█▎        | 9299/69539 [1:10:42<30:02:44,  1.80s/it] 13%|█▎        | 9300/69539 [1:10:44<28:51:17,  1.72s/it]                                                          13%|█▎        | 9300/69539 [1:10:44<28:51:17,  1.72s/it][INFO|trainer.py:2939] 2023-10-05 05:11:22,333 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9300
[INFO|trainer.py:3026] 2023-10-05 05:11:22,600 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9100] due to args.save_total_limit
 13%|█▎        | 9301/69539 [1:10:50<47:56:18,  2.86s/it] 13%|█▎        | 9302/69539 [1:10:51<43:05:58,  2.58s/it] 13%|█▎        | 9303/69539 [1:10:54<40:32:09,  2.42s/it] 13%|█▎        | 9304/69539 [1:10:55<35:24:44,  2.12s/it] 13%|█▎        | 9305/69539 [1:10:56<32:42:24,  1.95s/it] 13%|█▎        | 9306/69539 [1:10:58<30:02:14,  1.80s/it] 13%|█▎        | 9307/69539 [1:10:59<28:13:08,  1.69s/it] 13%|█▎        | 9308/69539 [1:11:01<28:03:39,  1.68s/it] 13%|█▎        | 9309/69539 [1:11:03<27:39:37,  1.65s/it] 13%|█▎        | 9310/69539 [1:11:05<29:46:12,  1.78s/it]                                                          13%|█▎        | 9310/69539 [1:11:05<29:46:12,  1.78s/it] 13%|█▎        | 9311/69539 [1:11:06<29:01:34,  1.73s/it] 13%|█▎        | 9312/69539 [1:11:08<27:26:49,  1.64s/it] 13%|█▎        | 9313/69539 [1:11:09<26:00:37,  1.55s/it] 13%|█▎        | 9314/69539 [1:11:11<27:00:29,  1.61s/it] 13%|█▎        | 9315/69539 [1:11:12<27:04:09,  1.62s/it] 13%|█▎        | 9316/69539 [1:11:14<28:16:08,  1.69s/it] 13%|█▎        | 9317/69539 [1:11:16<29:41:09,  1.77s/it] 13%|█▎        | 9318/69539 [1:11:18<28:16:29,  1.69s/it] 13%|█▎        | 9319/69539 [1:11:21<35:33:20,  2.13s/it] 13%|█▎        | 9320/69539 [1:11:26<51:38:27,  3.09s/it]                                                          13%|█▎        | 9320/69539 [1:11:26<51:38:27,  3.09s/it] 13%|█▎        | 9321/69539 [1:11:28<44:05:55,  2.64s/it] 13%|█▎        | 9322/69539 [1:11:29<37:47:31,  2.26s/it] 13%|█▎        | 9323/69539 [1:11:31<35:12:25,  2.10s/it] 13%|█▎        | 9324/69539 [1:11:33<32:55:09,  1.97s/it] 13%|█▎        | 9325/69539 [1:11:34<29:36:32,  1.77s/it] 13%|█▎        | 9326/69539 [1:11:35<27:13:04,  1.63s/it] 13%|█▎        | 9327/69539 [1:11:37<28:43:20,  1.72s/it] 13%|█▎        | 9328/69539 [1:11:38<26:40:31,  1.59s/it] 13%|█▎        | 9329/69539 [1:11:40<26:54:29,  1.61s/it] 13%|█▎        | 9330/69539 [1:11:42<26:11:54,  1.57s/it]                                                          13%|█▎        | 9330/69539 [1:11:42<26:11:54,  1.57s/it] 13%|█▎        | 9331/69539 [1:11:43<26:22:44,  1.58s/it] 13%|█▎        | 9332/69539 [1:11:45<27:03:34,  1.62s/it] 13%|█▎        | 9333/69539 [1:11:46<26:36:22,  1.59s/it] 13%|█▎        | 9334/69539 [1:11:48<25:47:30,  1.54s/it] 13%|█▎        | 9335/69539 [1:11:50<27:30:59,  1.65s/it] 13%|█▎        | 9336/69539 [1:11:51<26:55:44,  1.61s/it] 13%|█▎        | 9337/69539 [1:11:53<25:58:35,  1.55s/it] 13%|█▎        | 9338/69539 [1:11:54<25:04:44,  1.50s/it] 13%|█▎        | 9339/69539 [1:11:55<24:10:01,  1.45s/it] 13%|█▎        | 9340/69539 [1:11:57<24:35:09,  1.47s/it]                                                          13%|█▎        | 9340/69539 [1:11:57<24:35:09,  1.47s/it] 13%|█▎        | 9341/69539 [1:11:58<24:02:01,  1.44s/it] 13%|█▎        | 9342/69539 [1:12:00<23:17:41,  1.39s/it] 13%|█▎        | 9343/69539 [1:12:01<24:23:22,  1.46s/it] 13%|█▎        | 9344/69539 [1:12:03<24:01:09,  1.44s/it] 13%|█▎        | 9345/69539 [1:12:04<23:38:19,  1.41s/it] 13%|█▎        | 9346/69539 [1:12:06<25:03:06,  1.50s/it] 13%|█▎        | 9347/69539 [1:12:07<24:31:05,  1.47s/it] 13%|█▎        | 9348/69539 [1:12:09<24:47:21,  1.48s/it] 13%|█▎        | 9349/69539 [1:12:10<24:19:41,  1.46s/it] 13%|█▎        | 9350/69539 [1:12:11<24:54:10,  1.49s/it]                                                          13%|█▎        | 9350/69539 [1:12:11<24:54:10,  1.49s/it] 13%|█▎        | 9351/69539 [1:12:13<24:04:14,  1.44s/it] 13%|█▎        | 9352/69539 [1:12:14<24:01:14,  1.44s/it] 13%|█▎        | 9353/69539 [1:12:16<26:21:57,  1.58s/it] 13%|█▎        | 9354/69539 [1:12:18<26:31:38,  1.59s/it] 13%|█▎        | 9355/69539 [1:12:19<27:07:15,  1.62s/it] 13%|█▎        | 9356/69539 [1:12:21<27:36:01,  1.65s/it] 13%|█▎        | 9357/69539 [1:12:22<25:56:23,  1.55s/it] 13%|█▎        | 9358/69539 [1:12:24<26:43:40,  1.60s/it] 13%|█▎        | 9359/69539 [1:12:26<26:17:23,  1.57s/it] 13%|█▎        | 9360/69539 [1:12:27<25:51:45,  1.55s/it]                                                          13%|█▎        | 9360/69539 [1:12:27<25:51:45,  1.55s/it] 13%|█▎        | 9361/69539 [1:12:29<25:55:07,  1.55s/it] 13%|█▎        | 9362/69539 [1:12:30<25:37:19,  1.53s/it] 13%|█▎        | 9363/69539 [1:12:32<27:38:11,  1.65s/it] 13%|█▎        | 9364/69539 [1:12:34<27:26:12,  1.64s/it] 13%|█▎        | 9365/69539 [1:12:35<26:09:44,  1.57s/it] 13%|█▎        | 9366/69539 [1:12:37<25:04:44,  1.50s/it] 13%|█▎        | 9367/69539 [1:12:39<28:38:05,  1.71s/it] 13%|█▎        | 9368/69539 [1:12:40<28:36:58,  1.71s/it] 13%|█▎        | 9369/69539 [1:12:42<26:52:51,  1.61s/it] 13%|█▎        | 9370/69539 [1:12:43<25:59:53,  1.56s/it]                                                          13%|█▎        | 9370/69539 [1:12:43<25:59:53,  1.56s/it] 13%|█▎        | 9371/69539 [1:12:45<24:50:48,  1.49s/it] 13%|█▎        | 9372/69539 [1:12:46<25:06:11,  1.50s/it] 13%|█▎        | 9373/69539 [1:12:48<24:48:41,  1.48s/it] 13%|█▎        | 9374/69539 [1:12:49<24:22:31,  1.46s/it] 13%|█▎        | 9375/69539 [1:12:51<25:31:44,  1.53s/it] 13%|█▎        | 9376/69539 [1:12:52<24:30:26,  1.47s/it] 13%|█▎        | 9377/69539 [1:12:54<25:37:17,  1.53s/it] 13%|█▎        | 9378/69539 [1:12:56<31:55:17,  1.91s/it] 13%|█▎        | 9379/69539 [1:13:01<43:07:09,  2.58s/it] 13%|█▎        | 9380/69539 [1:13:02<37:51:37,  2.27s/it]                                                          13%|█▎        | 9380/69539 [1:13:02<37:51:37,  2.27s/it] 13%|█▎        | 9381/69539 [1:13:03<33:08:56,  1.98s/it] 13%|█▎        | 9382/69539 [1:13:07<40:41:59,  2.44s/it] 13%|█▎        | 9383/69539 [1:13:08<35:22:34,  2.12s/it] 13%|█▎        | 9384/69539 [1:13:10<31:19:54,  1.88s/it] 13%|█▎        | 9385/69539 [1:13:12<34:03:08,  2.04s/it] 13%|█▎        | 9386/69539 [1:13:14<33:23:27,  2.00s/it] 13%|█▎        | 9387/69539 [1:13:16<33:39:49,  2.01s/it] 14%|█▎        | 9388/69539 [1:13:18<34:54:10,  2.09s/it] 14%|█▎        | 9389/69539 [1:13:21<39:02:13,  2.34s/it] 14%|█▎        | 9390/69539 [1:13:23<34:55:33,  2.09s/it]                                                          14%|█▎        | 9390/69539 [1:13:23<34:55:33,  2.09s/it] 14%|█▎        | 9391/69539 [1:13:25<35:40:00,  2.13s/it] 14%|█▎        | 9392/69539 [1:13:27<37:26:50,  2.24s/it] 14%|█▎        | 9393/69539 [1:13:29<35:31:46,  2.13s/it] 14%|█▎        | 9394/69539 [1:13:31<33:05:32,  1.98s/it] 14%|█▎        | 9395/69539 [1:13:33<34:29:52,  2.06s/it] 14%|█▎        | 9396/69539 [1:13:35<32:20:58,  1.94s/it] 14%|█▎        | 9397/69539 [1:13:37<31:48:37,  1.90s/it] 14%|█▎        | 9398/69539 [1:13:39<33:03:27,  1.98s/it] 14%|█▎        | 9399/69539 [1:13:40<31:01:57,  1.86s/it] 14%|█▎        | 9400/69539 [1:13:42<27:55:45,  1.67s/it]                                                          14%|█▎        | 9400/69539 [1:13:42<27:55:45,  1.67s/it][INFO|trainer.py:2939] 2023-10-05 05:14:19,920 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9400
[INFO|trainer.py:3026] 2023-10-05 05:14:20,507 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_mt5_v1/checkpoint-9200] due to args.save_total_limit
 14%|█▎        | 9401/69539 [1:13:44<30:36:59,  1.83s/it] 14%|█▎        | 9402/69539 [1:13:45<28:54:45,  1.73s/it] 14%|█▎        | 9403/69539 [1:13:47<28:32:01,  1.71s/it] 14%|█▎        | 9404/69539 [1:13:48<27:24:10,  1.64s/it] 14%|█▎        | 9405/69539 [1:13:50<25:25:52,  1.52s/it] 14%|█▎        | 9406/69539 [1:13:53<34:15:53,  2.05s/it] 14%|█▎        | 9407/69539 [1:13:57<45:14:29,  2.71s/it] 14%|█▎        | 9408/69539 [1:14:00<45:38:08,  2.73s/it] 14%|█▎        | 9409/69539 [1:14:02<39:28:54,  2.36s/it] 14%|█▎        | 9410/69539 [1:14:03<35:23:18,  2.12s/it]                                                          14%|█▎        | 9410/69539 [1:14:03<35:23:18,  2.12s/it] 14%|█▎        | 9411/69539 [1:14:04<31:08:54,  1.86s/it] 14%|█▎        | 9412/69539 [1:14:07<34:40:41,  2.08s/it] 14%|█▎        | 9413/69539 [1:14:08<32:16:51,  1.93s/it] 14%|█▎        | 9414/69539 [1:14:10<29:41:49,  1.78s/it] 14%|█▎        | 9415/69539 [1:14:12<30:33:28,  1.83s/it] 14%|█▎        | 9416/69539 [1:14:14<31:32:54,  1.89s/it] 14%|█▎        | 9417/69539 [1:14:16<34:06:39,  2.04s/it] 14%|█▎        | 9418/69539 [1:14:18<33:14:48,  1.99s/it] 14%|█▎        | 9419/69539 [1:14:20<30:26:52,  1.82s/it] 14%|█▎        | 9420/69539 [1:14:21<28:38:43,  1.72s/it]                                                          14%|█▎        | 9420/69539 [1:14:21<28:38:43,  1.72s/it] 14%|█▎        | 9421/69539 [1:14:24<36:43:22,  2.20s/it] 14%|█▎        | 9422/69539 [1:14:26<36:16:22,  2.17s/it] 14%|█▎        | 9423/69539 [1:14:28<32:12:37,  1.93s/it] 14%|█▎        | 9424/69539 [1:14:30<33:42:30,  2.02s/it] 14%|█▎        | 9425/69539 [1:14:33<38:58:31,  2.33s/it] 14%|█▎        | 9426/69539 [1:14:35<35:04:54,  2.10s/it] 14%|█▎        | 9427/69539 [1:14:36<33:29:23,  2.01s/it] 14%|█▎        | 9428/69539 [1:14:39<37:11:24,  2.23s/it] 14%|█▎        | 9429/69539 [1:14:41<33:27:30,  2.00s/it] 14%|█▎        | 9430/69539 [1:14:43<35:46:56,  2.14s/it]                                                          14%|█▎        | 9430/69539 [1:14:43<35:46:56,  2.14s/it] 14%|█▎        | 9431/69539 [1:14:45<33:10:38,  1.99s/it] 14%|█▎        | 9432/69539 [1:14:47<34:33:27,  2.07s/it] 14%|█▎        | 9433/69539 [1:14:49<31:57:26,  1.91s/it] 14%|█▎        | 9434/69539 [1:14:51<36:28:55,  2.19s/it] 14%|█▎        | 9435/69539 [1:14:53<33:45:31,  2.02s/it] 14%|█▎        | 9436/69539 [1:14:55<32:03:06,  1.92s/it] 14%|█▎        | 9437/69539 [1:14:56<30:56:45,  1.85s/it] 14%|█▎        | 9438/69539 [1:14:58<30:35:21,  1.83s/it] 14%|█▎        | 9439/69539 [1:15:00<31:57:54,  1.91s/it] 14%|█▎        | 9440/69539 [1:15:02<30:05:16,  1.80s/it]                                                          14%|█▎        | 9440/69539 [1:15:02<30:05:16,  1.80s/it] 14%|█▎        | 9441/69539 [1:15:03<27:46:58,  1.66s/it] 14%|█▎        | 9442/69539 [1:15:05<27:02:04,  1.62s/it] 14%|█▎        | 9443/69539 [1:15:06<25:14:41,  1.51s/it] 14%|█▎        | 9444/69539 [1:15:07<24:21:50,  1.46s/it] 14%|█▎        | 9445/69539 [1:15:10<29:07:34,  1.74s/it] 14%|█▎        | 9446/69539 [1:15:12<32:13:17,  1.93s/it] 14%|█▎        | 9447/69539 [1:15:14<30:16:31,  1.81s/it] 14%|█▎        | 9448/69539 [1:15:15<28:26:25,  1.70s/it] 14%|█▎        | 9449/69539 [1:15:16<26:36:41,  1.59s/it] 14%|█▎        | 9450/69539 [1:15:18<27:35:16,  1.65s/it]                                                          14%|█▎        | 9450/69539 [1:15:18<27:35:16,  1.65s/it] 14%|█▎        | 9451/69539 [1:15:20<27:10:52,  1.63s/it] 14%|█▎        | 9452/69539 [1:15:22<28:52:56,  1.73s/it] 14%|█▎        | 9453/69539 [1:15:23<27:35:13,  1.65s/it] 14%|█▎        | 9454/69539 [1:15:26<31:23:58,  1.88s/it] 14%|█▎        | 9455/69539 [1:15:27<30:28:55,  1.83s/it] 14%|█▎        | 9456/69539 [1:15:29<30:37:04,  1.83s/it] 14%|█▎        | 9457/69539 [1:15:31<28:54:23,  1.73s/it] 14%|█▎        | 9458/69539 [1:15:33<34:02:58,  2.04s/it] 14%|█▎        | 9459/69539 [1:17:08<495:52:31, 29.71s/it] 14%|█▎        | 9460/69539 [1:23:15<2183:43:59, 130.85s/it]                                                             14%|█▎        | 9460/69539 [1:23:15<2183:43:59, 130.85s/it] 14%|█▎        | 9461/69539 [1:29:14<3329:50:30, 199.53s/it] 14%|█▎        | 9462/69539 [1:37:53<4925:46:42, 295.17s/it] 14%|█▎        | 9463/69539 [1:46:27<6022:28:21, 360.89s/it] 14%|█▎        | 9464/69539 [1:52:49<6129:41:33, 367.32s/it] 14%|█▎        | 9465/69539 [2:02:40<7248:04:08, 434.35s/it] 14%|█▎        | 9466/69539 [2:12:46<8108:56:37, 485.95s/it] 14%|█▎        | 9467/69539 [2:19:02<7559:01:39, 453.00s/it] 14%|█▎        | 9468/69539 [2:27:41<7884:40:23, 472.52s/it] 14%|█▎        | 9469/69539 [2:33:54<7389:38:02, 442.86s/it] 14%|█▎        | 9470/69539 [2:39:38<6893:25:12, 413.13s/it]                                                             14%|█▎        | 9470/69539 [2:39:38<6893:25:12, 413.13s/it] 14%|█▎        | 9471/69539 [2:57:56<10322:24:55, 618.64s/it] 14%|█▎        | 9472/69539 [3:18:18<13339:41:33, 799.49s/it] 14%|█▎        | 9473/69539 [3:39:33<15721:06:12, 942.23s/it]slurmstepd-idc-beta-batch-pvc-node-10: error: *** JOB 26793 ON idc-beta-batch-pvc-node-10 CANCELLED AT 2023-10-05T07:53:51 DUE TO TIME LIMIT ***
