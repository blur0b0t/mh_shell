starting fine tuning model
Requirement already satisfied: datasets in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.14.5)
Requirement already satisfied: torch in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.0.1)
Collecting transformers>=4.32.0
  Using cached transformers-4.33.2-py3-none-any.whl (7.6 MB)
Collecting sentencepiece
  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Requirement already satisfied: peft in /home/u131168/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.5.0)
Collecting evaluate
  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)
Collecting nltk
  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)
Collecting rouge_score
  Using cached rouge_score-0.1.2-py3-none-any.whl
Collecting einops
  Using cached einops-0.6.1-py3-none-any.whl (42 kB)
Requirement already satisfied: requests>=2.19.0 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.28.1)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.7)
Requirement already satisfied: xxhash in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.3.0)
Requirement already satisfied: tqdm>=4.62.1 in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (4.65.0)
Requirement already satisfied: multiprocess in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.15)
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.1)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.0)
Collecting huggingface-hub<1.0.0,>=0.14.0
  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 9.5 MB/s eta 0:00:00
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2023.6.0)
Requirement already satisfied: aiohttp in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (3.8.5)
Requirement already satisfied: packaging in /opt/miniconda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (23.0)
Requirement already satisfied: pandas in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (2.1.0)
Requirement already satisfied: pyarrow>=8.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 1)) (13.0.0)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.14.3)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: sympy in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)
Requirement already satisfied: networkx in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.9.0.58)
Requirement already satisfied: typing-extensions in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.8.0)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.10.3.66)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.4.0.1)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.12.4)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.91)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.101)
Requirement already satisfied: jinja2 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.2)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)
Requirement already satisfied: triton==2.0.0 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.0.0)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (8.5.0.96)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (10.2.10.91)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/u131168/.local/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (11.7.4.91)
Requirement already satisfied: setuptools in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (65.6.3)
Requirement already satisfied: wheel in /opt/miniconda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (0.38.4)
Requirement already satisfied: lit in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (16.0.6)
Requirement already satisfied: cmake in /home/u131168/.local/lib/python3.10/site-packages (from triton==2.0.0->torch->-r requirements.txt (line 2)) (3.27.5)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (0.3.3)
Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers>=4.32.0->-r requirements.txt (line 3)) (2023.8.8)
Requirement already satisfied: psutil in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (5.9.5)
Requirement already satisfied: accelerate in /home/u131168/.local/lib/python3.10/site-packages (from peft->-r requirements.txt (line 5)) (0.23.0)
Collecting responses<0.19
  Using cached responses-0.18.0-py3-none-any.whl (38 kB)
Collecting joblib
  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)
Collecting click
  Using cached click-8.1.7-py3-none-any.whl (97 kB)
Requirement already satisfied: six>=1.14.0 in /opt/miniconda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 8)) (1.16.0)
Collecting absl-py
  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.9.2)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.0.4)
Requirement already satisfied: attrs>=17.3.0 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/miniconda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.0.4)
Requirement already satisfied: frozenlist>=1.1.1 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)
Requirement already satisfied: aiosignal>=1.1.2 in /home/u131168/.local/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (2023.5.7)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 1)) (1.26.15)
Requirement already satisfied: MarkupSafe>=2.0 in /home/u131168/.local/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)
Requirement already satisfied: python-dateutil>=2.8.2 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /home/u131168/.local/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2023.3)
Requirement already satisfied: mpmath>=0.19 in /home/u131168/.local/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)
Installing collected packages: tokenizers, sentencepiece, joblib, einops, click, absl-py, responses, nltk, huggingface-hub, transformers, rouge_score, evaluate
Successfully installed absl-py-2.0.0 click-8.1.7 einops-0.6.1 evaluate-0.4.0 huggingface-hub-0.17.3 joblib-1.3.2 nltk-3.8.1 responses-0.18.0 rouge_score-0.1.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.33.2
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-41d9bzj5
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-41d9bzj5
  Resolved https://github.com/huggingface/transformers to commit 408b2b3c5057b275855ae4c43c452a7f0b37aa45
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: pyyaml>=5.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (6.0.1)
Requirement already satisfied: filelock in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (3.12.4)
Requirement already satisfied: safetensors>=0.3.1 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (0.3.3)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (0.17.3)
Requirement already satisfied: numpy>=1.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (1.26.0)
Requirement already satisfied: packaging>=20.0 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (23.0)
Requirement already satisfied: tqdm>=4.27 in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (4.65.0)
Collecting tokenizers<0.15,>=0.14
  Using cached tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)
Requirement already satisfied: regex!=2019.12.17 in /home/u131168/.local/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (2023.8.8)
Requirement already satisfied: requests in /opt/miniconda/lib/python3.10/site-packages (from transformers==4.34.0.dev0) (2.28.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.8.0)
Requirement already satisfied: fsspec in /home/u131168/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)
Collecting huggingface-hub<1.0,>=0.16.4
  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.34.0.dev0) (1.26.15)
Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.34.0.dev0) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.34.0.dev0) (2023.5.7)
Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda/lib/python3.10/site-packages (from requests->transformers==4.34.0.dev0) (2.0.4)
Building wheels for collected packages: transformers
  Building wheel for transformers (pyproject.toml): started
  Building wheel for transformers (pyproject.toml): finished with status 'done'
  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7728521 sha256=214119e64ab07d5ecf18df9c5278403d6ed7eb577985ce0a069b74fb42228b3e
  Stored in directory: /tmp/pip-ephem-wheel-cache-qq8dp86m/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b
Successfully built transformers
Installing collected packages: huggingface-hub, tokenizers, transformers
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.17.3
    Uninstalling huggingface-hub-0.17.3:
      Successfully uninstalled huggingface-hub-0.17.3
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.13.3
    Uninstalling tokenizers-0.13.3:
      Successfully uninstalled tokenizers-0.13.3
  Attempting uninstall: transformers
    Found existing installation: transformers 4.33.2
    Uninstalling transformers-4.33.2:
      Successfully uninstalled transformers-4.33.2
Successfully installed huggingface-hub-0.16.4 tokenizers-0.14.0 transformers-4.34.0.dev0
/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6400
-------------------starting-script
-------------------setting up-logging
09/26/2023 23:53:48 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0distributed training: True, 16-bits training: False
09/26/2023 23:53:48 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/runs/Sep26_23-53-48_idc-beta-batch-pvc-node-13,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=30.0,
optim=adamw_torch,
optim_args=None,
output_dir=/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6400,
run_name=/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model,
save_on_each_node=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=2,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.03,
warmup_steps=0,
weight_decay=0.0,
)
/home/u131168/.local/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
Using custom data configuration default-9a367dcac09ea6c0
----------------------detecting checkpoint
09/26/2023 23:53:48 - INFO - datasets.builder - Using custom data configuration default-9a367dcac09ea6c0
Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
09/26/2023 23:53:48 - INFO - datasets.info - Loading Dataset Infos from /home/u131168/.local/lib/python3.10/site-packages/datasets/packaged_modules/csv
Overwrite dataset info from restored data version if exists.
09/26/2023 23:53:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
09/26/2023 23:53:48 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
09/26/2023 23:53:48 - INFO - datasets.builder - Found cached dataset csv (/home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)
Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
09/26/2023 23:53:48 - INFO - datasets.info - Loading Dataset info from /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d
[INFO|tokenization_utils_base.py:2037] 2023-09-26 23:53:48,616 >> loading file spiece.model from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/spiece.model
[INFO|tokenization_utils_base.py:2037] 2023-09-26 23:53:48,616 >> loading file tokenizer.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer.json
[INFO|tokenization_utils_base.py:2037] 2023-09-26 23:53:48,616 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2037] 2023-09-26 23:53:48,616 >> loading file special_tokens_map.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/special_tokens_map.json
[INFO|tokenization_utils_base.py:2037] 2023-09-26 23:53:48,616 >> loading file tokenizer_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/tokenizer_config.json
[WARNING|logging.py:305] 2023-09-26 23:53:48,616 >> Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
Loading cached processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-164b05582cde9085.arrow
09/26/2023 23:53:48 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/u131168/.cache/huggingface/datasets/csv/default-9a367dcac09ea6c0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-164b05582cde9085.arrow
[INFO|configuration_utils.py:715] 2023-09-26 23:53:49,257 >> loading configuration file config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/config.json
[INFO|configuration_utils.py:775] 2023-09-26 23:53:49,260 >> Model config T5Config {
  "_name_or_path": "google/flan-t5-xl",
  "architectures": [
    "T5ForConditionalGeneration"
  ],
  "classifier_dropout": 0.0,
  "d_ff": 5120,
  "d_kv": 64,
  "d_model": 2048,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 24,
  "num_heads": 32,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.34.0.dev0",
  "use_cache": true,
  "vocab_size": 32128
}

[INFO|modeling_utils.py:2975] 2023-09-26 23:53:49,276 >> loading weights file pytorch_model.bin from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/pytorch_model.bin.index.json
[INFO|configuration_utils.py:770] 2023-09-26 23:53:49,288 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  8.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.86s/it]
[INFO|modeling_utils.py:3757] 2023-09-26 23:54:21,498 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.

[INFO|modeling_utils.py:3765] 2023-09-26 23:54:21,499 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-xl.
If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:730] 2023-09-26 23:54:21,603 >> loading configuration file generation_config.json from cache at /home/u131168/.cache/huggingface/hub/models--google--flan-t5-xl/snapshots/8772db7a7a11f7b08e6be7d7088f7a7fd4813bc5/generation_config.json
[INFO|configuration_utils.py:770] 2023-09-26 23:54:21,603 >> Generate config GenerationConfig {
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0
}

[WARNING|modeling_utils.py:1617] 2023-09-26 23:54:21,624 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
[INFO|trainer.py:2123] 2023-09-26 23:54:25,985 >> Loading model from /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6400.
[INFO|trainer.py:1760] 2023-09-26 23:54:26,439 >> ***** Running training *****
[INFO|trainer.py:1761] 2023-09-26 23:54:26,439 >>   Num examples = 1,377
[INFO|trainer.py:1762] 2023-09-26 23:54:26,439 >>   Num Epochs = 30
[INFO|trainer.py:1763] 2023-09-26 23:54:26,439 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:1766] 2023-09-26 23:54:26,439 >>   Total train batch size (w. parallel, distributed & accumulation) = 2
[INFO|trainer.py:1767] 2023-09-26 23:54:26,439 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1768] 2023-09-26 23:54:26,439 >>   Total optimization steps = 20,670
[INFO|trainer.py:1769] 2023-09-26 23:54:26,442 >>   Number of trainable parameters = 4,718,592
[INFO|trainer.py:1789] 2023-09-26 23:54:26,445 >>   Continuing training from checkpoint, will skip to saved global_step
[INFO|trainer.py:1790] 2023-09-26 23:54:26,445 >>   Continuing training from epoch 9
[INFO|trainer.py:1791] 2023-09-26 23:54:26,445 >>   Continuing training from global step 6400
[INFO|trainer.py:1793] 2023-09-26 23:54:26,445 >>   Will skip the first 9 epochs then the first 199 batches in the first epoch.
trainable params: 4,718,592 || all params: 2,854,361,088 || trainable%: 0.1653116706164963
-------------------traing-model
  0%|          | 0/20670 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-09-26 23:54:26,462 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 31%|███       | 6401/20670 [00:01<00:03, 3673.40it/s] 31%|███       | 6406/20670 [00:12<00:03, 3673.40it/s] 31%|███       | 6407/20670 [00:14<00:44, 320.15it/s]  31%|███       | 6408/20670 [00:17<00:55, 255.03it/s]                                                      31%|███       | 6410/20670 [00:26<00:55, 255.03it/s] 31%|███       | 6412/20670 [00:32<00:55, 255.03it/s] 31%|███       | 6413/20670 [00:34<02:47, 85.03it/s]  31%|███       | 6414/20670 [00:36<03:05, 76.93it/s]                                                     31%|███       | 6420/20670 [00:52<03:05, 76.93it/s] 31%|███       | 6420/20670 [00:52<03:05, 76.93it/s] 31%|███       | 6421/20670 [00:55<07:18, 32.52it/s] 31%|███       | 6422/20670 [00:57<08:00, 29.64it/s] 31%|███       | 6429/20670 [01:12<08:00, 29.64it/s] 31%|███       | 6430/20670 [01:14<15:01, 15.79it/s]                                                     31%|███       | 6430/20670 [01:14<15:01, 15.79it/s] 31%|███       | 6431/20670 [01:16<16:25, 14.45it/s] 31%|███       | 6435/20670 [01:32<16:25, 14.45it/s] 31%|███       | 6436/20670 [01:34<31:44,  7.47it/s] 31%|███       | 6437/20670 [01:36<34:12,  6.94it/s]                                                     31%|███       | 6440/20670 [01:43<34:11,  6.94it/s] 31%|███       | 6441/20670 [01:52<34:11,  6.94it/s] 31%|███       | 6442/20670 [01:53<1:02:02,  3.82it/s] 31%|███       | 6443/20670 [01:55<1:06:39,  3.56it/s]                                                       31%|███       | 6450/20670 [02:11<1:06:37,  3.56it/s] 31%|███       | 6451/20670 [02:12<1:06:36,  3.56it/s] 31%|███       | 6452/20670 [02:15<1:59:42,  1.98it/s] 31%|███       | 6453/20670 [02:17<2:07:13,  1.86it/s] 31%|███       | 6459/20670 [02:32<2:07:10,  1.86it/s] 31%|███▏      | 6460/20670 [02:34<3:13:05,  1.23it/s]                                                       31%|███▏      | 6460/20670 [02:34<3:13:05,  1.23it/s] 31%|███▏      | 6461/20670 [02:37<3:27:12,  1.14it/s] 31%|███▏      | 6466/20670 [02:52<3:27:08,  1.14it/s] 31%|███▏      | 6467/20670 [02:54<5:09:25,  1.31s/it] 31%|███▏      | 6468/20670 [02:58<5:30:02,  1.39s/it]                                                       31%|███▏      | 6470/20670 [03:02<5:30:00,  1.39s/it] 31%|███▏      | 6473/20670 [03:11<6:48:55,  1.73s/it] 31%|███▏      | 6474/20670 [03:14<6:59:53,  1.77s/it] 31%|███▏      | 6478/20670 [03:21<7:08:46,  1.81s/it]                                                       31%|███▏      | 6480/20670 [03:26<7:08:43,  1.81s/it] 31%|███▏      | 6481/20670 [03:27<7:18:25,  1.85s/it] 31%|███▏      | 6483/20670 [03:31<7:20:32,  1.86s/it] 31%|███▏      | 6485/20670 [03:35<7:26:40,  1.89s/it] 31%|███▏      | 6486/20670 [03:37<7:29:04,  1.90s/it] 31%|███▏      | 6487/20670 [03:39<7:44:57,  1.97s/it] 31%|███▏      | 6488/20670 [03:42<7:59:14,  2.03s/it] 31%|███▏      | 6489/20670 [03:44<8:02:11,  2.04s/it] 31%|███▏      | 6490/20670 [03:46<7:53:50,  2.00s/it]                                                       31%|███▏      | 6490/20670 [03:46<7:53:50,  2.00s/it] 31%|███▏      | 6491/20670 [03:48<8:14:09,  2.09s/it] 31%|███▏      | 6492/20670 [03:51<8:53:53,  2.26s/it] 31%|███▏      | 6493/20670 [03:52<8:00:53,  2.04s/it] 31%|███▏      | 6494/20670 [03:53<6:58:23,  1.77s/it] 31%|███▏      | 6495/20670 [03:54<6:12:50,  1.58s/it] 31%|███▏      | 6496/20670 [03:56<6:38:25,  1.69s/it] 31%|███▏      | 6497/20670 [03:58<5:59:54,  1.52s/it] 31%|███▏      | 6498/20670 [03:59<5:43:55,  1.46s/it] 31%|███▏      | 6499/20670 [04:00<5:24:38,  1.37s/it] 31%|███▏      | 6500/20670 [04:01<5:29:00,  1.39s/it]                                                       31%|███▏      | 6500/20670 [04:01<5:29:00,  1.39s/it][INFO|trainer.py:2939] 2023-09-26 23:58:28,380 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6500
[INFO|trainer.py:3026] 2023-09-26 23:58:28,609 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6300] due to args.save_total_limit
 31%|███▏      | 6501/20670 [04:03<5:33:31,  1.41s/it] 31%|███▏      | 6502/20670 [04:04<5:19:25,  1.35s/it] 31%|███▏      | 6503/20670 [04:05<5:04:34,  1.29s/it] 31%|███▏      | 6504/20670 [04:07<5:46:56,  1.47s/it] 31%|███▏      | 6505/20670 [04:08<5:23:27,  1.37s/it] 31%|███▏      | 6506/20670 [04:09<4:59:59,  1.27s/it] 31%|███▏      | 6507/20670 [04:11<5:01:31,  1.28s/it] 31%|███▏      | 6508/20670 [04:12<5:08:54,  1.31s/it] 31%|███▏      | 6509/20670 [04:14<5:34:58,  1.42s/it] 31%|███▏      | 6510/20670 [04:16<6:31:21,  1.66s/it]                                                       31%|███▏      | 6510/20670 [04:16<6:31:21,  1.66s/it] 31%|███▏      | 6511/20670 [04:18<6:47:32,  1.73s/it] 32%|███▏      | 6512/20670 [04:19<6:14:04,  1.59s/it] 32%|███▏      | 6513/20670 [04:20<5:52:55,  1.50s/it] 32%|███▏      | 6514/20670 [04:23<6:50:26,  1.74s/it] 32%|███▏      | 6515/20670 [04:25<7:01:50,  1.79s/it] 32%|███▏      | 6516/20670 [04:26<6:23:32,  1.63s/it] 32%|███▏      | 6517/20670 [04:27<6:01:51,  1.53s/it] 32%|███▏      | 6518/20670 [04:29<6:05:50,  1.55s/it] 32%|███▏      | 6519/20670 [04:31<6:54:31,  1.76s/it] 32%|███▏      | 6520/20670 [04:33<7:21:43,  1.87s/it]                                                       32%|███▏      | 6520/20670 [04:33<7:21:43,  1.87s/it] 32%|███▏      | 6521/20670 [04:34<6:36:35,  1.68s/it] 32%|███▏      | 6522/20670 [04:36<7:04:11,  1.80s/it] 32%|███▏      | 6523/20670 [04:38<7:16:09,  1.85s/it] 32%|███▏      | 6524/20670 [04:40<6:46:01,  1.72s/it] 32%|███▏      | 6525/20670 [04:42<6:53:37,  1.75s/it] 32%|███▏      | 6526/20670 [04:44<7:24:21,  1.89s/it] 32%|███▏      | 6527/20670 [04:46<7:54:27,  2.01s/it] 32%|███▏      | 6528/20670 [04:48<8:03:52,  2.05s/it] 32%|███▏      | 6529/20670 [04:50<8:05:46,  2.06s/it] 32%|███▏      | 6530/20670 [04:52<7:35:16,  1.93s/it]                                                       32%|███▏      | 6530/20670 [04:52<7:35:16,  1.93s/it] 32%|███▏      | 6531/20670 [04:54<7:32:08,  1.92s/it] 32%|███▏      | 6532/20670 [04:56<7:51:30,  2.00s/it] 32%|███▏      | 6533/20670 [04:58<7:33:52,  1.93s/it] 32%|███▏      | 6534/20670 [04:59<7:02:18,  1.79s/it] 32%|███▏      | 6535/20670 [05:02<7:36:57,  1.94s/it] 32%|███▏      | 6536/20670 [05:04<7:39:42,  1.95s/it] 32%|███▏      | 6537/20670 [05:05<7:35:02,  1.93s/it] 32%|███▏      | 6538/20670 [05:09<9:02:57,  2.31s/it] 32%|███▏      | 6539/20670 [05:11<9:26:28,  2.41s/it] 32%|███▏      | 6540/20670 [05:13<8:44:22,  2.23s/it]                                                       32%|███▏      | 6540/20670 [05:13<8:44:22,  2.23s/it] 32%|███▏      | 6541/20670 [05:15<7:57:03,  2.03s/it] 32%|███▏      | 6542/20670 [05:16<7:46:13,  1.98s/it] 32%|███▏      | 6543/20670 [05:18<7:44:58,  1.97s/it] 32%|███▏      | 6544/20670 [05:20<7:43:36,  1.97s/it] 32%|███▏      | 6545/20670 [05:22<7:16:17,  1.85s/it] 32%|███▏      | 6546/20670 [05:23<6:48:47,  1.74s/it] 32%|███▏      | 6547/20670 [05:25<6:47:06,  1.73s/it] 32%|███▏      | 6548/20670 [05:27<6:53:25,  1.76s/it] 32%|███▏      | 6549/20670 [05:29<6:48:00,  1.73s/it] 32%|███▏      | 6550/20670 [05:30<6:52:11,  1.75s/it]                                                       32%|███▏      | 6550/20670 [05:30<6:52:11,  1.75s/it] 32%|███▏      | 6551/20670 [05:32<6:29:03,  1.65s/it] 32%|███▏      | 6552/20670 [05:33<6:26:05,  1.64s/it] 32%|███▏      | 6553/20670 [05:35<6:35:11,  1.68s/it] 32%|███▏      | 6554/20670 [05:37<6:37:12,  1.69s/it] 32%|███▏      | 6555/20670 [05:39<6:33:49,  1.67s/it] 32%|███▏      | 6556/20670 [05:40<6:47:14,  1.73s/it] 32%|███▏      | 6557/20670 [05:42<6:40:49,  1.70s/it] 32%|███▏      | 6558/20670 [05:44<6:44:36,  1.72s/it] 32%|███▏      | 6559/20670 [05:45<6:23:35,  1.63s/it] 32%|███▏      | 6560/20670 [05:47<6:57:12,  1.77s/it]                                                       32%|███▏      | 6560/20670 [05:47<6:57:12,  1.77s/it] 32%|███▏      | 6561/20670 [05:49<6:57:48,  1.78s/it] 32%|███▏      | 6562/20670 [05:51<6:39:08,  1.70s/it] 32%|███▏      | 6563/20670 [05:52<6:43:38,  1.72s/it] 32%|███▏      | 6564/20670 [05:54<6:47:40,  1.73s/it] 32%|███▏      | 6565/20670 [05:56<6:28:58,  1.65s/it] 32%|███▏      | 6566/20670 [05:57<6:23:59,  1.63s/it] 32%|███▏      | 6567/20670 [05:59<6:40:24,  1.70s/it] 32%|███▏      | 6568/20670 [06:01<6:30:57,  1.66s/it] 32%|███▏      | 6569/20670 [06:02<6:16:06,  1.60s/it] 32%|███▏      | 6570/20670 [06:04<6:12:08,  1.58s/it]                                                       32%|███▏      | 6570/20670 [06:04<6:12:08,  1.58s/it] 32%|███▏      | 6571/20670 [06:05<6:18:56,  1.61s/it] 32%|███▏      | 6572/20670 [06:07<6:38:20,  1.70s/it] 32%|███▏      | 6573/20670 [06:09<6:47:27,  1.73s/it] 32%|███▏      | 6574/20670 [06:11<6:48:07,  1.74s/it] 32%|███▏      | 6575/20670 [06:12<6:35:12,  1.68s/it] 32%|███▏      | 6576/20670 [06:14<6:36:01,  1.69s/it] 32%|███▏      | 6577/20670 [06:15<6:02:54,  1.55s/it] 32%|███▏      | 6578/20670 [06:17<6:10:55,  1.58s/it] 32%|███▏      | 6579/20670 [06:19<6:20:31,  1.62s/it] 32%|███▏      | 6580/20670 [06:20<6:21:59,  1.63s/it]                                                       32%|███▏      | 6580/20670 [06:20<6:21:59,  1.63s/it] 32%|███▏      | 6581/20670 [06:22<6:58:02,  1.78s/it] 32%|███▏      | 6582/20670 [06:24<7:02:25,  1.80s/it] 32%|███▏      | 6583/20670 [06:26<7:09:03,  1.83s/it] 32%|███▏      | 6584/20670 [06:28<7:33:53,  1.93s/it] 32%|███▏      | 6585/20670 [06:31<7:48:44,  2.00s/it] 32%|███▏      | 6586/20670 [06:33<8:02:49,  2.06s/it] 32%|███▏      | 6587/20670 [06:34<6:53:02,  1.76s/it] 32%|███▏      | 6588/20670 [06:36<7:53:45,  2.02s/it] 32%|███▏      | 6589/20670 [06:38<7:51:09,  2.01s/it] 32%|███▏      | 6590/20670 [06:41<8:00:47,  2.05s/it]                                                       32%|███▏      | 6590/20670 [06:41<8:00:47,  2.05s/it] 32%|███▏      | 6591/20670 [06:43<8:14:18,  2.11s/it] 32%|███▏      | 6592/20670 [06:45<8:20:06,  2.13s/it] 32%|███▏      | 6593/20670 [06:47<8:21:59,  2.14s/it] 32%|███▏      | 6594/20670 [06:49<8:19:45,  2.13s/it] 32%|███▏      | 6595/20670 [06:51<8:01:44,  2.05s/it] 32%|███▏      | 6596/20670 [06:52<7:02:55,  1.80s/it] 32%|███▏      | 6597/20670 [06:55<7:36:34,  1.95s/it] 32%|███▏      | 6598/20670 [06:56<7:28:24,  1.91s/it] 32%|███▏      | 6599/20670 [06:58<7:38:43,  1.96s/it] 32%|███▏      | 6600/20670 [07:00<7:41:51,  1.97s/it]                                                       32%|███▏      | 6600/20670 [07:00<7:41:51,  1.97s/it][INFO|trainer.py:2939] 2023-09-27 00:01:27,449 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6600
[INFO|trainer.py:3026] 2023-09-27 00:01:27,685 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6400] due to args.save_total_limit
 32%|███▏      | 6601/20670 [07:03<8:50:37,  2.26s/it] 32%|███▏      | 6602/20670 [07:06<8:46:35,  2.25s/it] 32%|███▏      | 6603/20670 [07:08<9:06:25,  2.33s/it] 32%|███▏      | 6604/20670 [07:11<9:18:35,  2.38s/it] 32%|███▏      | 6605/20670 [07:12<8:33:56,  2.19s/it] 32%|███▏      | 6606/20670 [07:15<8:27:19,  2.16s/it] 32%|███▏      | 6607/20670 [07:17<8:43:08,  2.23s/it] 32%|███▏      | 6608/20670 [07:19<8:30:28,  2.18s/it] 32%|███▏      | 6609/20670 [07:21<7:57:15,  2.04s/it] 32%|███▏      | 6610/20670 [07:23<8:14:06,  2.11s/it]                                                       32%|███▏      | 6610/20670 [07:23<8:14:06,  2.11s/it] 32%|███▏      | 6611/20670 [07:25<8:43:38,  2.23s/it] 32%|███▏      | 6612/20670 [07:27<7:25:13,  1.90s/it] 32%|███▏      | 6613/20670 [07:28<6:50:11,  1.75s/it] 32%|███▏      | 6614/20670 [07:30<7:21:28,  1.88s/it] 32%|███▏      | 6615/20670 [07:32<7:30:44,  1.92s/it] 32%|███▏      | 6616/20670 [07:34<7:36:42,  1.95s/it] 32%|███▏      | 6617/20670 [07:36<7:43:43,  1.98s/it] 32%|███▏      | 6618/20670 [07:38<7:36:57,  1.95s/it] 32%|███▏      | 6619/20670 [07:40<7:36:53,  1.95s/it] 32%|███▏      | 6620/20670 [07:42<7:54:07,  2.02s/it]                                                       32%|███▏      | 6620/20670 [07:42<7:54:07,  2.02s/it] 32%|███▏      | 6621/20670 [07:44<7:28:12,  1.91s/it] 32%|███▏      | 6622/20670 [07:46<7:05:19,  1.82s/it] 32%|███▏      | 6623/20670 [07:47<7:03:13,  1.81s/it] 32%|███▏      | 6624/20670 [07:49<6:44:34,  1.73s/it] 32%|███▏      | 6625/20670 [07:51<7:17:45,  1.87s/it] 32%|███▏      | 6626/20670 [07:53<6:54:54,  1.77s/it] 32%|███▏      | 6627/20670 [07:54<6:47:57,  1.74s/it] 32%|███▏      | 6628/20670 [07:57<7:29:49,  1.92s/it] 32%|███▏      | 6629/20670 [07:58<6:56:03,  1.78s/it] 32%|███▏      | 6630/20670 [07:59<6:29:18,  1.66s/it]                                                       32%|███▏      | 6630/20670 [07:59<6:29:18,  1.66s/it] 32%|███▏      | 6631/20670 [08:01<6:24:10,  1.64s/it] 32%|███▏      | 6632/20670 [08:03<6:20:23,  1.63s/it] 32%|███▏      | 6633/20670 [08:05<7:07:56,  1.83s/it] 32%|███▏      | 6634/20670 [08:07<7:05:59,  1.82s/it] 32%|███▏      | 6635/20670 [08:08<6:57:38,  1.79s/it] 32%|███▏      | 6636/20670 [08:11<7:18:12,  1.87s/it] 32%|███▏      | 6637/20670 [08:13<7:43:45,  1.98s/it] 32%|███▏      | 6638/20670 [08:15<8:25:03,  2.16s/it] 32%|███▏      | 6639/20670 [08:17<7:37:29,  1.96s/it] 32%|███▏      | 6640/20670 [08:19<7:18:20,  1.87s/it]                                                       32%|███▏      | 6640/20670 [08:19<7:18:20,  1.87s/it] 32%|███▏      | 6641/20670 [08:20<6:55:32,  1.78s/it] 32%|███▏      | 6642/20670 [08:22<6:39:52,  1.71s/it] 32%|███▏      | 6643/20670 [08:23<6:27:20,  1.66s/it] 32%|███▏      | 6644/20670 [08:25<6:15:19,  1.61s/it] 32%|███▏      | 6645/20670 [08:26<6:11:35,  1.59s/it] 32%|███▏      | 6646/20670 [08:28<6:03:32,  1.56s/it] 32%|███▏      | 6647/20670 [08:29<6:02:48,  1.55s/it] 32%|███▏      | 6648/20670 [08:32<6:59:05,  1.79s/it] 32%|███▏      | 6649/20670 [08:34<7:20:28,  1.88s/it] 32%|███▏      | 6650/20670 [08:36<8:06:33,  2.08s/it]                                                       32%|███▏      | 6650/20670 [08:36<8:06:33,  2.08s/it] 32%|███▏      | 6651/20670 [08:38<7:36:00,  1.95s/it] 32%|███▏      | 6652/20670 [08:40<8:20:25,  2.14s/it] 32%|███▏      | 6653/20670 [08:42<7:54:23,  2.03s/it] 32%|███▏      | 6654/20670 [08:44<7:27:32,  1.92s/it] 32%|███▏      | 6655/20670 [08:46<7:09:46,  1.84s/it] 32%|███▏      | 6656/20670 [08:47<6:47:53,  1.75s/it] 32%|███▏      | 6657/20670 [08:49<7:00:25,  1.80s/it] 32%|███▏      | 6658/20670 [08:52<7:53:07,  2.03s/it] 32%|███▏      | 6659/20670 [08:53<7:43:33,  1.99s/it] 32%|███▏      | 6660/20670 [08:56<8:07:34,  2.09s/it]                                                       32%|███▏      | 6660/20670 [08:56<8:07:34,  2.09s/it] 32%|███▏      | 6661/20670 [08:58<8:21:49,  2.15s/it] 32%|███▏      | 6662/20670 [09:00<8:00:36,  2.06s/it] 32%|███▏      | 6663/20670 [09:02<8:29:37,  2.18s/it] 32%|███▏      | 6664/20670 [09:04<8:11:15,  2.10s/it] 32%|███▏      | 6665/20670 [09:06<8:17:55,  2.13s/it] 32%|███▏      | 6666/20670 [09:08<8:05:38,  2.08s/it] 32%|███▏      | 6667/20670 [09:10<7:44:13,  1.99s/it] 32%|███▏      | 6668/20670 [09:12<8:02:04,  2.07s/it] 32%|███▏      | 6669/20670 [09:14<7:48:47,  2.01s/it] 32%|███▏      | 6670/20670 [09:16<7:39:22,  1.97s/it]                                                       32%|███▏      | 6670/20670 [09:16<7:39:22,  1.97s/it] 32%|███▏      | 6671/20670 [09:18<7:33:13,  1.94s/it] 32%|███▏      | 6672/20670 [09:20<7:33:21,  1.94s/it] 32%|███▏      | 6673/20670 [09:22<7:00:30,  1.80s/it] 32%|███▏      | 6674/20670 [09:23<7:07:14,  1.83s/it] 32%|███▏      | 6675/20670 [09:26<7:30:36,  1.93s/it] 32%|███▏      | 6676/20670 [09:28<7:43:03,  1.99s/it] 32%|███▏      | 6677/20670 [09:30<7:52:17,  2.03s/it] 32%|███▏      | 6678/20670 [09:33<8:40:21,  2.23s/it] 32%|███▏      | 6679/20670 [09:35<8:40:37,  2.23s/it] 32%|███▏      | 6680/20670 [09:37<8:30:57,  2.19s/it]                                                       32%|███▏      | 6680/20670 [09:37<8:30:57,  2.19s/it] 32%|███▏      | 6681/20670 [09:40<9:37:17,  2.48s/it] 32%|███▏      | 6682/20670 [09:42<9:17:38,  2.39s/it] 32%|███▏      | 6683/20670 [09:44<8:52:20,  2.28s/it] 32%|███▏      | 6684/20670 [09:46<8:23:46,  2.16s/it] 32%|███▏      | 6685/20670 [09:48<8:14:17,  2.12s/it] 32%|███▏      | 6686/20670 [09:50<8:05:46,  2.08s/it] 32%|███▏      | 6687/20670 [09:52<8:20:10,  2.15s/it] 32%|███▏      | 6688/20670 [09:55<8:25:40,  2.17s/it] 32%|███▏      | 6689/20670 [09:57<8:53:44,  2.29s/it] 32%|███▏      | 6690/20670 [09:59<8:22:29,  2.16s/it]                                                       32%|███▏      | 6690/20670 [09:59<8:22:29,  2.16s/it] 32%|███▏      | 6691/20670 [10:01<7:59:52,  2.06s/it] 32%|███▏      | 6692/20670 [10:04<8:55:07,  2.30s/it] 32%|███▏      | 6693/20670 [10:06<8:19:26,  2.14s/it] 32%|███▏      | 6694/20670 [10:08<8:41:51,  2.24s/it] 32%|███▏      | 6695/20670 [10:10<8:46:11,  2.26s/it] 32%|███▏      | 6696/20670 [10:12<8:24:51,  2.17s/it] 32%|███▏      | 6697/20670 [10:14<8:06:26,  2.09s/it] 32%|███▏      | 6698/20670 [10:16<8:06:02,  2.09s/it] 32%|███▏      | 6699/20670 [10:18<8:12:38,  2.12s/it] 32%|███▏      | 6700/20670 [10:21<8:24:52,  2.17s/it]                                                       32%|███▏      | 6700/20670 [10:21<8:24:52,  2.17s/it][INFO|trainer.py:2939] 2023-09-27 00:04:47,667 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6700
[INFO|trainer.py:3026] 2023-09-27 00:04:47,902 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6500] due to args.save_total_limit
 32%|███▏      | 6701/20670 [10:23<9:05:51,  2.34s/it] 32%|███▏      | 6702/20670 [10:26<9:28:31,  2.44s/it] 32%|███▏      | 6703/20670 [10:28<8:57:32,  2.31s/it] 32%|███▏      | 6704/20670 [10:30<8:40:54,  2.24s/it] 32%|███▏      | 6705/20670 [10:32<8:12:27,  2.12s/it] 32%|███▏      | 6706/20670 [10:34<8:05:46,  2.09s/it] 32%|███▏      | 6707/20670 [10:36<7:51:40,  2.03s/it] 32%|███▏      | 6708/20670 [10:38<8:04:08,  2.08s/it] 32%|███▏      | 6709/20670 [10:40<7:55:04,  2.04s/it] 32%|███▏      | 6710/20670 [10:42<8:00:44,  2.07s/it]                                                       32%|███▏      | 6710/20670 [10:42<8:00:44,  2.07s/it] 32%|███▏      | 6711/20670 [10:44<7:11:50,  1.86s/it] 32%|███▏      | 6712/20670 [10:46<8:04:49,  2.08s/it] 32%|███▏      | 6713/20670 [10:48<7:45:12,  2.00s/it] 32%|███▏      | 6714/20670 [10:50<7:33:04,  1.95s/it] 32%|███▏      | 6715/20670 [10:52<7:42:31,  1.99s/it] 32%|███▏      | 6716/20670 [10:55<8:28:50,  2.19s/it] 32%|███▏      | 6717/20670 [10:57<8:12:20,  2.12s/it] 33%|███▎      | 6718/20670 [10:59<8:08:46,  2.10s/it] 33%|███▎      | 6719/20670 [11:01<8:01:26,  2.07s/it] 33%|███▎      | 6720/20670 [11:03<8:49:49,  2.28s/it]                                                       33%|███▎      | 6720/20670 [11:03<8:49:49,  2.28s/it] 33%|███▎      | 6721/20670 [11:05<8:33:44,  2.21s/it] 33%|███▎      | 6722/20670 [11:07<8:14:51,  2.13s/it] 33%|███▎      | 6723/20670 [11:10<8:31:04,  2.20s/it] 33%|███▎      | 6724/20670 [11:12<8:26:49,  2.18s/it] 33%|███▎      | 6725/20670 [11:14<8:34:08,  2.21s/it] 33%|███▎      | 6726/20670 [11:17<8:59:56,  2.32s/it] 33%|███▎      | 6727/20670 [11:19<8:39:23,  2.24s/it] 33%|███▎      | 6728/20670 [11:20<7:44:59,  2.00s/it] 33%|███▎      | 6729/20670 [11:22<7:31:28,  1.94s/it] 33%|███▎      | 6730/20670 [11:24<7:37:12,  1.97s/it]                                                       33%|███▎      | 6730/20670 [11:24<7:37:12,  1.97s/it] 33%|███▎      | 6731/20670 [11:26<7:43:09,  1.99s/it] 33%|███▎      | 6732/20670 [11:28<7:48:33,  2.02s/it] 33%|███▎      | 6733/20670 [11:29<6:53:37,  1.78s/it] 33%|███▎      | 6734/20670 [11:31<6:52:08,  1.77s/it] 33%|███▎      | 6735/20670 [11:33<7:23:48,  1.91s/it] 33%|███▎      | 6736/20670 [11:36<7:49:42,  2.02s/it] 33%|███▎      | 6737/20670 [11:38<7:54:27,  2.04s/it] 33%|███▎      | 6738/20670 [11:40<7:38:45,  1.98s/it] 33%|███▎      | 6739/20670 [11:42<7:43:29,  2.00s/it] 33%|███▎      | 6740/20670 [11:44<7:50:55,  2.03s/it]                                                       33%|███▎      | 6740/20670 [11:44<7:50:55,  2.03s/it] 33%|███▎      | 6741/20670 [11:46<8:01:07,  2.07s/it] 33%|███▎      | 6742/20670 [11:48<8:05:28,  2.09s/it] 33%|███▎      | 6743/20670 [11:49<7:19:21,  1.89s/it] 33%|███▎      | 6744/20670 [11:52<7:51:46,  2.03s/it] 33%|███▎      | 6745/20670 [11:54<7:55:14,  2.05s/it] 33%|███▎      | 6746/20670 [11:56<7:46:12,  2.01s/it] 33%|███▎      | 6747/20670 [11:58<8:08:03,  2.10s/it] 33%|███▎      | 6748/20670 [12:01<8:43:05,  2.25s/it] 33%|███▎      | 6749/20670 [12:03<9:03:35,  2.34s/it] 33%|███▎      | 6750/20670 [12:05<8:26:30,  2.18s/it]                                                       33%|███▎      | 6750/20670 [12:05<8:26:30,  2.18s/it] 33%|███▎      | 6751/20670 [12:07<8:25:10,  2.18s/it] 33%|███▎      | 6752/20670 [12:09<8:12:44,  2.12s/it] 33%|███▎      | 6753/20670 [12:11<7:55:03,  2.05s/it] 33%|███▎      | 6754/20670 [12:13<7:52:18,  2.04s/it] 33%|███▎      | 6755/20670 [12:15<7:47:54,  2.02s/it] 33%|███▎      | 6756/20670 [12:17<8:05:49,  2.10s/it] 33%|███▎      | 6757/20670 [12:19<8:00:25,  2.07s/it] 33%|███▎      | 6758/20670 [12:21<7:55:25,  2.05s/it] 33%|███▎      | 6759/20670 [12:23<7:45:09,  2.01s/it] 33%|███▎      | 6760/20670 [12:25<7:55:38,  2.05s/it]                                                       33%|███▎      | 6760/20670 [12:25<7:55:38,  2.05s/it] 33%|███▎      | 6761/20670 [12:28<8:02:36,  2.08s/it] 33%|███▎      | 6762/20670 [12:30<8:57:16,  2.32s/it] 33%|███▎      | 6763/20670 [12:33<8:41:51,  2.25s/it] 33%|███▎      | 6764/20670 [12:34<8:15:54,  2.14s/it] 33%|███▎      | 6765/20670 [12:37<8:29:27,  2.20s/it] 33%|███▎      | 6766/20670 [12:38<7:23:06,  1.91s/it] 33%|███▎      | 6767/20670 [12:40<7:28:02,  1.93s/it] 33%|███▎      | 6768/20670 [12:43<8:06:24,  2.10s/it] 33%|███▎      | 6769/20670 [12:45<7:58:43,  2.07s/it] 33%|███▎      | 6770/20670 [12:46<7:34:35,  1.96s/it]                                                       33%|███▎      | 6770/20670 [12:46<7:34:35,  1.96s/it] 33%|███▎      | 6771/20670 [12:48<7:45:19,  2.01s/it] 33%|███▎      | 6772/20670 [12:50<7:53:22,  2.04s/it] 33%|███▎      | 6773/20670 [12:53<8:09:56,  2.12s/it] 33%|███▎      | 6774/20670 [12:55<7:52:05,  2.04s/it] 33%|███▎      | 6775/20670 [12:57<7:58:27,  2.07s/it] 33%|███▎      | 6776/20670 [12:59<8:27:39,  2.19s/it] 33%|███▎      | 6777/20670 [13:02<8:56:57,  2.32s/it] 33%|███▎      | 6778/20670 [13:04<8:44:19,  2.26s/it] 33%|███▎      | 6779/20670 [13:06<8:47:30,  2.28s/it] 33%|███▎      | 6780/20670 [13:08<8:39:23,  2.24s/it]                                                       33%|███▎      | 6780/20670 [13:08<8:39:23,  2.24s/it] 33%|███▎      | 6781/20670 [13:11<8:25:21,  2.18s/it] 33%|███▎      | 6782/20670 [13:13<8:14:13,  2.14s/it] 33%|███▎      | 6783/20670 [13:15<8:14:30,  2.14s/it] 33%|███▎      | 6784/20670 [13:17<7:58:26,  2.07s/it] 33%|███▎      | 6785/20670 [13:19<8:06:06,  2.10s/it] 33%|███▎      | 6786/20670 [13:21<8:00:41,  2.08s/it] 33%|███▎      | 6787/20670 [13:22<7:20:54,  1.91s/it] 33%|███▎      | 6788/20670 [13:25<7:43:22,  2.00s/it] 33%|███▎      | 6789/20670 [13:26<7:33:26,  1.96s/it] 33%|███▎      | 6790/20670 [13:28<7:24:22,  1.92s/it]                                                       33%|███▎      | 6790/20670 [13:28<7:24:22,  1.92s/it] 33%|███▎      | 6791/20670 [13:30<7:01:10,  1.82s/it] 33%|███▎      | 6792/20670 [13:31<6:16:47,  1.63s/it] 33%|███▎      | 6793/20670 [13:33<7:02:18,  1.83s/it] 33%|███▎      | 6794/20670 [13:37<8:45:28,  2.27s/it] 33%|███▎      | 6795/20670 [13:38<8:17:02,  2.15s/it] 33%|███▎      | 6796/20670 [13:41<8:41:37,  2.26s/it] 33%|███▎      | 6797/20670 [13:43<8:25:30,  2.19s/it] 33%|███▎      | 6798/20670 [13:45<8:10:30,  2.12s/it] 33%|███▎      | 6799/20670 [13:46<7:29:42,  1.95s/it] 33%|███▎      | 6800/20670 [13:48<6:43:36,  1.75s/it]                                                       33%|███▎      | 6800/20670 [13:48<6:43:36,  1.75s/it][INFO|trainer.py:2939] 2023-09-27 00:08:14,693 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6800
[INFO|trainer.py:3026] 2023-09-27 00:08:14,921 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6600] due to args.save_total_limit
 33%|███▎      | 6801/20670 [13:50<7:26:14,  1.93s/it] 33%|███▎      | 6802/20670 [13:52<7:16:34,  1.89s/it] 33%|███▎      | 6803/20670 [13:54<7:37:28,  1.98s/it] 33%|███▎      | 6804/20670 [13:56<7:44:40,  2.01s/it] 33%|███▎      | 6805/20670 [13:59<8:07:54,  2.11s/it] 33%|███▎      | 6806/20670 [14:01<8:06:12,  2.10s/it] 33%|███▎      | 6807/20670 [14:03<7:56:08,  2.06s/it] 33%|███▎      | 6808/20670 [14:05<8:10:39,  2.12s/it] 33%|███▎      | 6809/20670 [14:07<8:23:40,  2.18s/it] 33%|███▎      | 6810/20670 [14:09<8:11:01,  2.13s/it]                                                       33%|███▎      | 6810/20670 [14:09<8:11:01,  2.13s/it] 33%|███▎      | 6811/20670 [14:11<8:00:34,  2.08s/it] 33%|███▎      | 6812/20670 [14:14<8:36:07,  2.23s/it] 33%|███▎      | 6813/20670 [14:15<7:26:07,  1.93s/it] 33%|███▎      | 6814/20670 [14:17<7:11:39,  1.87s/it] 33%|███▎      | 6815/20670 [14:18<6:34:57,  1.71s/it] 33%|███▎      | 6816/20670 [14:20<6:56:26,  1.80s/it] 33%|███▎      | 6817/20670 [14:22<7:07:27,  1.85s/it] 33%|███▎      | 6818/20670 [14:24<7:25:42,  1.93s/it] 33%|███▎      | 6819/20670 [14:27<8:01:40,  2.09s/it] 33%|███▎      | 6820/20670 [14:29<8:07:30,  2.11s/it]                                                       33%|███▎      | 6820/20670 [14:29<8:07:30,  2.11s/it] 33%|███▎      | 6821/20670 [14:31<8:24:20,  2.19s/it] 33%|███▎      | 6822/20670 [14:33<8:04:33,  2.10s/it] 33%|███▎      | 6823/20670 [14:35<7:57:54,  2.07s/it] 33%|███▎      | 6824/20670 [14:37<7:56:38,  2.07s/it] 33%|███▎      | 6825/20670 [14:39<7:53:51,  2.05s/it] 33%|███▎      | 6826/20670 [14:41<7:20:54,  1.91s/it] 33%|███▎      | 6827/20670 [14:43<7:28:43,  1.94s/it] 33%|███▎      | 6828/20670 [14:44<6:54:45,  1.80s/it] 33%|███▎      | 6829/20670 [14:46<7:24:04,  1.93s/it] 33%|███▎      | 6830/20670 [14:49<7:51:55,  2.05s/it]                                                       33%|███▎      | 6830/20670 [14:49<7:51:55,  2.05s/it] 33%|███▎      | 6831/20670 [14:50<7:16:32,  1.89s/it] 33%|███▎      | 6832/20670 [14:53<8:11:41,  2.13s/it] 33%|███▎      | 6833/20670 [14:55<8:00:56,  2.09s/it] 33%|███▎      | 6834/20670 [14:56<7:23:44,  1.92s/it] 33%|███▎      | 6835/20670 [14:58<7:34:06,  1.97s/it] 33%|███▎      | 6836/20670 [15:00<7:33:53,  1.97s/it] 33%|███▎      | 6837/20670 [15:02<7:04:08,  1.84s/it] 33%|███▎      | 6838/20670 [15:04<6:41:58,  1.74s/it] 33%|███▎      | 6839/20670 [15:05<6:27:45,  1.68s/it] 33%|███▎      | 6840/20670 [15:07<6:15:54,  1.63s/it]                                                       33%|███▎      | 6840/20670 [15:07<6:15:54,  1.63s/it] 33%|███▎      | 6841/20670 [15:08<5:49:07,  1.51s/it] 33%|███▎      | 6842/20670 [15:09<5:21:33,  1.40s/it] 33%|███▎      | 6843/20670 [15:10<5:29:29,  1.43s/it] 33%|███▎      | 6844/20670 [15:12<5:42:23,  1.49s/it] 33%|███▎      | 6845/20670 [15:14<5:50:03,  1.52s/it] 33%|███▎      | 6846/20670 [15:15<6:00:04,  1.56s/it] 33%|███▎      | 6847/20670 [15:18<7:11:38,  1.87s/it] 33%|███▎      | 6848/20670 [15:20<7:31:58,  1.96s/it] 33%|███▎      | 6849/20670 [15:22<6:58:54,  1.82s/it] 33%|███▎      | 6850/20670 [15:23<6:42:39,  1.75s/it]                                                       33%|███▎      | 6850/20670 [15:23<6:42:39,  1.75s/it] 33%|███▎      | 6851/20670 [15:25<6:58:41,  1.82s/it] 33%|███▎      | 6852/20670 [15:27<7:28:00,  1.95s/it] 33%|███▎      | 6853/20670 [15:29<7:04:46,  1.84s/it] 33%|███▎      | 6854/20670 [15:31<7:17:56,  1.90s/it] 33%|███▎      | 6855/20670 [15:33<7:49:47,  2.04s/it] 33%|███▎      | 6856/20670 [15:35<7:14:43,  1.89s/it] 33%|███▎      | 6857/20670 [15:37<7:22:55,  1.92s/it] 33%|███▎      | 6858/20670 [15:38<6:51:29,  1.79s/it] 33%|███▎      | 6859/20670 [15:41<7:21:39,  1.92s/it] 33%|███▎      | 6860/20670 [15:42<6:46:06,  1.76s/it]                                                       33%|███▎      | 6860/20670 [15:42<6:46:06,  1.76s/it] 33%|███▎      | 6861/20670 [15:44<7:21:32,  1.92s/it] 33%|███▎      | 6862/20670 [15:46<6:55:04,  1.80s/it] 33%|███▎      | 6863/20670 [15:48<6:52:57,  1.79s/it] 33%|███▎      | 6864/20670 [15:49<6:50:08,  1.78s/it] 33%|███▎      | 6865/20670 [15:52<7:41:59,  2.01s/it] 33%|███▎      | 6866/20670 [15:54<7:31:27,  1.96s/it] 33%|███▎      | 6867/20670 [15:56<7:32:19,  1.97s/it] 33%|███▎      | 6868/20670 [15:58<7:29:01,  1.95s/it] 33%|███▎      | 6869/20670 [16:00<7:31:09,  1.96s/it] 33%|███▎      | 6870/20670 [16:02<7:45:17,  2.02s/it]                                                       33%|███▎      | 6870/20670 [16:02<7:45:17,  2.02s/it] 33%|███▎      | 6871/20670 [16:04<7:45:07,  2.02s/it] 33%|███▎      | 6872/20670 [16:06<7:28:18,  1.95s/it] 33%|███▎      | 6873/20670 [16:07<7:22:31,  1.92s/it] 33%|███▎      | 6874/20670 [16:09<7:23:47,  1.93s/it] 33%|███▎      | 6875/20670 [16:11<7:28:54,  1.95s/it] 33%|███▎      | 6876/20670 [16:13<7:30:46,  1.96s/it] 33%|███▎      | 6877/20670 [16:15<7:31:09,  1.96s/it] 33%|███▎      | 6878/20670 [16:17<7:27:17,  1.95s/it] 33%|███▎      | 6879/20670 [16:19<7:39:28,  2.00s/it] 33%|███▎      | 6880/20670 [16:21<7:36:49,  1.99s/it]                                                       33%|███▎      | 6880/20670 [16:21<7:36:49,  1.99s/it] 33%|███▎      | 6881/20670 [16:23<7:46:24,  2.03s/it] 33%|███▎      | 6882/20670 [16:26<7:53:06,  2.06s/it] 33%|███▎      | 6883/20670 [16:28<7:46:16,  2.03s/it] 33%|███▎      | 6884/20670 [16:29<7:04:47,  1.85s/it] 33%|███▎      | 6885/20670 [16:31<7:32:17,  1.97s/it] 33%|███▎      | 6886/20670 [16:33<7:44:42,  2.02s/it] 33%|███▎      | 6887/20670 [16:35<7:44:05,  2.02s/it] 33%|███▎      | 6888/20670 [16:38<8:20:27,  2.18s/it] 33%|███▎      | 6889/20670 [16:40<8:28:49,  2.22s/it] 33%|███▎      | 6890/20670 [16:41<7:12:02,  1.88s/it]                                                       33%|███▎      | 6890/20670 [16:41<7:12:02,  1.88s/it] 33%|███▎      | 6891/20670 [16:44<8:34:48,  2.24s/it] 33%|███▎      | 6892/20670 [16:46<8:12:48,  2.15s/it] 33%|███▎      | 6893/20670 [16:49<8:37:39,  2.25s/it] 33%|███▎      | 6894/20670 [16:51<8:34:14,  2.24s/it] 33%|███▎      | 6895/20670 [16:53<8:27:02,  2.21s/it] 33%|███▎      | 6896/20670 [16:56<8:33:08,  2.24s/it] 33%|███▎      | 6897/20670 [16:58<8:21:59,  2.19s/it] 33%|███▎      | 6898/20670 [17:00<8:57:38,  2.34s/it] 33%|███▎      | 6899/20670 [17:02<8:39:31,  2.26s/it] 33%|███▎      | 6900/20670 [17:04<8:29:37,  2.22s/it]                                                       33%|███▎      | 6900/20670 [17:04<8:29:37,  2.22s/it][INFO|trainer.py:2939] 2023-09-27 00:11:31,435 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6900
[INFO|trainer.py:3026] 2023-09-27 00:11:31,684 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6700] due to args.save_total_limit
 33%|███▎      | 6901/20670 [17:06<8:04:13,  2.11s/it] 33%|███▎      | 6902/20670 [17:08<7:51:09,  2.05s/it] 33%|███▎      | 6903/20670 [17:11<8:30:20,  2.22s/it] 33%|███▎      | 6904/20670 [17:13<8:14:43,  2.16s/it] 33%|███▎      | 6905/20670 [17:15<8:06:04,  2.12s/it] 33%|███▎      | 6906/20670 [17:17<7:49:24,  2.05s/it] 33%|███▎      | 6907/20670 [17:19<8:00:58,  2.10s/it] 33%|███▎      | 6908/20670 [17:22<9:25:22,  2.46s/it] 33%|███▎      | 6909/20670 [17:24<8:56:25,  2.34s/it] 33%|███▎      | 6910/20670 [17:26<8:39:13,  2.26s/it]                                                       33%|███▎      | 6910/20670 [17:26<8:39:13,  2.26s/it] 33%|███▎      | 6911/20670 [17:29<8:48:28,  2.30s/it] 33%|███▎      | 6912/20670 [17:31<8:22:44,  2.19s/it] 33%|███▎      | 6913/20670 [17:33<8:20:10,  2.18s/it] 33%|███▎      | 6914/20670 [17:36<9:06:36,  2.38s/it] 33%|███▎      | 6915/20670 [17:38<8:40:09,  2.27s/it] 33%|███▎      | 6916/20670 [17:40<8:20:39,  2.18s/it] 33%|███▎      | 6917/20670 [17:42<8:19:07,  2.18s/it] 33%|███▎      | 6918/20670 [17:43<7:24:14,  1.94s/it] 33%|███▎      | 6919/20670 [17:45<7:39:10,  2.00s/it] 33%|███▎      | 6920/20670 [17:48<7:59:35,  2.09s/it]                                                       33%|███▎      | 6920/20670 [17:48<7:59:35,  2.09s/it] 33%|███▎      | 6921/20670 [17:50<7:53:44,  2.07s/it] 33%|███▎      | 6922/20670 [17:52<7:48:46,  2.05s/it] 33%|███▎      | 6923/20670 [17:54<8:10:45,  2.14s/it] 33%|███▎      | 6924/20670 [17:56<8:17:12,  2.17s/it] 34%|███▎      | 6925/20670 [17:59<8:18:03,  2.17s/it] 34%|███▎      | 6926/20670 [18:01<8:09:01,  2.13s/it] 34%|███▎      | 6927/20670 [18:03<8:03:48,  2.11s/it] 34%|███▎      | 6928/20670 [18:05<7:56:20,  2.08s/it] 34%|███▎      | 6929/20670 [18:07<8:22:44,  2.20s/it] 34%|███▎      | 6930/20670 [18:09<8:16:54,  2.17s/it]                                                       34%|███▎      | 6930/20670 [18:09<8:16:54,  2.17s/it] 34%|███▎      | 6931/20670 [18:11<8:21:05,  2.19s/it] 34%|███▎      | 6932/20670 [18:14<8:40:39,  2.27s/it] 34%|███▎      | 6933/20670 [18:15<7:22:11,  1.93s/it] 34%|███▎      | 6934/20670 [18:17<7:21:39,  1.93s/it] 34%|███▎      | 6935/20670 [18:19<7:23:26,  1.94s/it] 34%|███▎      | 6936/20670 [18:20<6:27:57,  1.69s/it] 34%|███▎      | 6937/20670 [18:22<6:49:44,  1.79s/it] 34%|███▎      | 6938/20670 [18:24<7:09:55,  1.88s/it] 34%|███▎      | 6939/20670 [18:25<6:24:20,  1.68s/it] 34%|███▎      | 6940/20670 [18:27<6:49:31,  1.79s/it]                                                       34%|███▎      | 6940/20670 [18:27<6:49:31,  1.79s/it] 34%|███▎      | 6941/20670 [18:30<7:08:35,  1.87s/it] 34%|███▎      | 6942/20670 [18:32<7:23:57,  1.94s/it] 34%|███▎      | 6943/20670 [18:34<7:50:17,  2.06s/it] 34%|███▎      | 6944/20670 [18:36<7:23:22,  1.94s/it] 34%|███▎      | 6945/20670 [18:38<7:38:29,  2.00s/it] 34%|███▎      | 6946/20670 [18:40<7:49:41,  2.05s/it] 34%|███▎      | 6947/20670 [18:42<7:42:39,  2.02s/it] 34%|███▎      | 6948/20670 [18:44<8:08:21,  2.14s/it] 34%|███▎      | 6949/20670 [18:47<8:36:24,  2.26s/it] 34%|███▎      | 6950/20670 [18:49<8:53:49,  2.33s/it]                                                       34%|███▎      | 6950/20670 [18:49<8:53:49,  2.33s/it] 34%|███▎      | 6951/20670 [18:52<8:51:52,  2.33s/it] 34%|███▎      | 6952/20670 [18:54<9:01:55,  2.37s/it] 34%|███▎      | 6953/20670 [18:56<9:01:05,  2.37s/it] 34%|███▎      | 6954/20670 [18:59<8:50:36,  2.32s/it] 34%|███▎      | 6955/20670 [19:00<7:34:41,  1.99s/it] 34%|███▎      | 6956/20670 [19:02<7:39:10,  2.01s/it] 34%|███▎      | 6957/20670 [19:05<8:27:42,  2.22s/it] 34%|███▎      | 6958/20670 [19:07<8:11:49,  2.15s/it] 34%|███▎      | 6959/20670 [19:09<8:07:21,  2.13s/it] 34%|███▎      | 6960/20670 [19:11<8:10:15,  2.15s/it]                                                       34%|███▎      | 6960/20670 [19:11<8:10:15,  2.15s/it] 34%|███▎      | 6961/20670 [19:13<8:10:40,  2.15s/it] 34%|███▎      | 6962/20670 [19:15<7:55:04,  2.08s/it] 34%|███▎      | 6963/20670 [19:17<8:00:55,  2.11s/it] 34%|███▎      | 6964/20670 [19:19<7:58:38,  2.10s/it] 34%|███▎      | 6965/20670 [19:21<7:58:42,  2.10s/it] 34%|███▎      | 6966/20670 [19:23<7:53:35,  2.07s/it] 34%|███▎      | 6967/20670 [19:26<7:59:15,  2.10s/it] 34%|███▎      | 6968/20670 [19:29<9:02:14,  2.37s/it] 34%|███▎      | 6969/20670 [19:31<8:47:59,  2.31s/it] 34%|███▎      | 6970/20670 [19:33<8:45:54,  2.30s/it]                                                       34%|███▎      | 6970/20670 [19:33<8:45:54,  2.30s/it] 34%|███▎      | 6971/20670 [19:35<7:55:40,  2.08s/it] 34%|███▎      | 6972/20670 [19:37<7:58:13,  2.09s/it] 34%|███▎      | 6973/20670 [19:39<7:52:56,  2.07s/it] 34%|███▎      | 6974/20670 [19:40<7:22:03,  1.94s/it] 34%|███▎      | 6975/20670 [19:42<7:18:25,  1.92s/it] 34%|███▎      | 6976/20670 [19:43<6:27:20,  1.70s/it] 34%|███▍      | 6977/20670 [19:45<6:38:11,  1.74s/it] 34%|███▍      | 6978/20670 [19:47<6:44:23,  1.77s/it] 34%|███▍      | 6979/20670 [19:49<7:12:29,  1.90s/it] 34%|███▍      | 6980/20670 [19:51<7:31:05,  1.98s/it]                                                       34%|███▍      | 6980/20670 [19:51<7:31:05,  1.98s/it] 34%|███▍      | 6981/20670 [19:54<7:58:44,  2.10s/it] 34%|███▍      | 6982/20670 [19:56<8:19:04,  2.19s/it] 34%|███▍      | 6983/20670 [19:58<8:05:30,  2.13s/it] 34%|███▍      | 6984/20670 [20:01<8:31:33,  2.24s/it] 34%|███▍      | 6985/20670 [20:04<9:27:25,  2.49s/it] 34%|███▍      | 6986/20670 [20:06<8:54:30,  2.34s/it] 34%|███▍      | 6987/20670 [20:08<9:07:32,  2.40s/it] 34%|███▍      | 6988/20670 [20:11<9:48:26,  2.58s/it] 34%|███▍      | 6989/20670 [20:41<41:04:46, 10.81s/it] 34%|███▍      | 6990/20670 [20:44<31:21:21,  8.25s/it]                                                        34%|███▍      | 6990/20670 [20:44<31:21:21,  8.25s/it] 34%|███▍      | 6991/20670 [25:42<361:59:59, 95.27s/it] 34%|███▍      | 6992/20670 [26:44<323:47:03, 85.22s/it] 34%|███▍      | 6993/20670 [30:44<500:51:11, 131.83s/it] 34%|███▍      | 6994/20670 [35:44<692:12:07, 182.21s/it] 34%|███▍      | 6995/20670 [43:27<1011:31:08, 266.29s/it] 34%|███▍      | 6996/20670 [45:44<864:08:01, 227.50s/it]  34%|███▍      | 6997/20670 [55:41<1285:22:01, 338.43s/it] 34%|███▍      | 6998/20670 [58:41<1105:18:19, 291.04s/it] 34%|███▍      | 6999/20670 [1:00:44<913:22:30, 240.52s/it] 34%|███▍      | 7000/20670 [1:05:41<977:13:41, 257.35s/it]                                                            34%|███▍      | 7000/20670 [1:05:41<977:13:41, 257.35s/it][INFO|trainer.py:2939] 2023-09-27 01:00:07,456 >> Saving model checkpoint to /home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-7000
[INFO|trainer.py:3026] 2023-09-27 01:00:07,697 >> Deleting older checkpoint [/home/u131168/mh_shell/ft_models/flan-t5-xl_peft_finetuned_model/checkpoint-6800] due to args.save_total_limit
 34%|███▍      | 7001/20670 [1:08:18<863:03:12, 227.30s/it] 34%|███▍      | 7002/20670 [1:10:44<770:33:30, 202.96s/it] 34%|███▍      | 7003/20670 [1:20:13<1187:23:53, 312.77s/it] 34%|███▍      | 7004/20670 [1:20:44<866:36:21, 228.29s/it]  34%|███▍      | 7005/20670 [1:30:10<1251:35:14, 329.73s/it] 34%|███▍      | 7006/20670 [1:30:44<913:43:21, 240.73s/it]  34%|███▍      | 7007/20670 [1:35:34<970:36:56, 255.74s/it] 34%|███▍      | 7008/20670 [1:35:44<690:49:00, 182.03s/it] 34%|███▍      | 7009/20670 [1:40:43<823:13:29, 216.94s/it] 34%|███▍      | 7010/20670 [1:48:07<1082:02:49, 285.17s/it]                                                             34%|███▍      | 7010/20670 [1:48:07<1082:02:49, 285.17s/it] 34%|███▍      | 7011/20670 [1:50:44<935:36:39, 246.59s/it]  34%|███▍      | 7012/20670 [1:59:37<1262:21:35, 332.74s/it] 34%|███▍      | 7013/20670 [2:00:44<959:18:28, 252.87s/it]  34%|███▍      | 7014/20670 [2:09:19<1257:53:18, 331.61s/it] 34%|███▍      | 7015/20670 [2:10:44<976:22:05, 257.41s/it]  34%|███▍      | 7016/20670 [2:20:41<1363:19:32, 359.45s/it] 34%|███▍      | 7017/20670 [2:20:44<957:34:05, 252.49s/it]  34%|███▍      | 7018/20670 [2:26:22<1055:20:13, 278.29s/it] 34%|███▍      | 7019/20670 [2:31:50<1110:52:05, 292.95s/it] 34%|███▍      | 7020/20670 [2:35:45<1044:49:53, 275.56s/it]                                                             34%|███▍      | 7020/20670 [2:35:45<1044:49:53, 275.56s/it] 34%|███▍      | 7021/20670 [2:42:27<1188:56:29, 313.59s/it] 34%|███▍      | 7022/20670 [2:45:43<1054:52:29, 278.25s/it] 34%|███▍      | 7023/20670 [2:50:43<1080:02:35, 284.91s/it] 34%|███▍      | 7024/20670 [2:59:46<1372:58:38, 362.21s/it] 34%|███▍      | 7025/20670 [3:00:44<1027:00:52, 270.96s/it] 34%|███▍      | 7026/20670 [3:06:17<1097:34:08, 289.60s/it] 34%|███▍      | 7027/20670 [3:11:24<1117:25:41, 294.86s/it] 34%|███▍      | 7028/20670 [3:15:43<1076:58:40, 284.20s/it] 34%|███▍      | 7029/20670 [3:25:41<1432:47:32, 378.13s/it] 34%|███▍      | 7030/20670 [3:25:44<1006:36:33, 265.67s/it]                                                             34%|███▍      | 7030/20670 [3:25:44<1006:36:33, 265.67s/it] 34%|███▍      | 7031/20670 [3:34:13<1283:27:41, 338.77s/it] 34%|███▍      | 7032/20670 [3:35:44<1001:48:51, 264.45s/it] 34%|███▍      | 7033/20670 [3:43:51<1253:50:46, 331.00s/it] 34%|███▍      | 7034/20670 [3:45:44<1005:59:18, 265.59s/it] 34%|███▍      | 7035/20670 [3:54:32<1304:20:35, 344.38s/it] 34%|███▍      | 7036/20670 [3:55:44<995:22:34, 262.82s/it] slurmstepd-idc-beta-batch-pvc-node-13: error: *** JOB 22563 ON idc-beta-batch-pvc-node-13 CANCELLED AT 2023-09-27T03:53:39 DUE TO TIME LIMIT ***
